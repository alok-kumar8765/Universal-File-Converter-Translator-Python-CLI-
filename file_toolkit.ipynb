{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CJH3er8eORM"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cechDnE320ZJ"
      },
      "outputs": [],
      "source": [
        "#requirements.txt\n",
        "!pip install pdfplumber python-docx pandas  pdf2docx openpyxl PyPDF2 pillow googletrans==4.0.0-rc1 pytesseract pdf2image pytest\n",
        "!pip install deep-translator\n",
        "!apt-get install pandoc\n",
        "!pip install pypandoc\n",
        "!pip install reportlab\n",
        "!apt-get -y install wkhtmltopdf\n",
        "!pip install pdfkit\n",
        "!pip install langdetect\n",
        "!pip install langid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj-3ywZ-eWsH"
      },
      "source": [
        "# File_toolkit_colab_v2.py  \n",
        "## Note : * Only Translation Part working fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aWJVoEB13HMb"
      },
      "outputs": [],
      "source": [
        "# Install deps first in Colab\n",
        "# !pip install pypandoc pdfplumber python-docx pandas openpyxl pillow deep-translator\n",
        "# pandoc download will happen automatically inside code\n",
        "\n",
        "import os\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "from PIL import Image, ImageDraw\n",
        "import pypandoc\n",
        "from deep_translator import GoogleTranslator\n",
        "from google.colab import files\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "# Ensure pandoc available\n",
        "try:\n",
        "    pypandoc.get_pandoc_version()\n",
        "except OSError:\n",
        "    print(\"⚡ Installing Pandoc ...\")\n",
        "    pypandoc.download_pandoc()\n",
        "\n",
        "# ---------- FILE CONVERSION ----------\n",
        "def pdf_to_text(file_path, out_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    return out_path\n",
        "\n",
        "def doc_to_text(file_path, out_path):\n",
        "    doc = Document(file_path)\n",
        "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    return out_path\n",
        "\n",
        "def text_to_image(file_path, out_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    img = Image.new(\"RGB\", (1200, 1600), \"white\")\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.text((40, 40), text, fill=\"black\")\n",
        "    img.save(out_path)\n",
        "    return out_path\n",
        "\n",
        "def csv_to_xls(file_path, out_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.to_excel(out_path, index=False)\n",
        "    return out_path\n",
        "\n",
        "def txt_to_pdf(input_path, output_path):\n",
        "    c = canvas.Canvas(output_path, pagesize=letter)\n",
        "    width, height = letter\n",
        "\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    x, y = 40, height - 40\n",
        "    for line in lines:\n",
        "        if y < 40:  # new page if space finished\n",
        "            c.showPage()\n",
        "            y = height - 40\n",
        "        c.drawString(x, y, line.strip())\n",
        "        y -= 15\n",
        "\n",
        "    c.save()\n",
        "    print(f\"✅ PDF saved at {output_path}\")\n",
        "    #return out_path\n",
        "\n",
        "\n",
        "def generic_convert(in_file, out_file, format_to):\n",
        "    ext = os.path.splitext(in_file)[1].lower()\n",
        "    # For TXT input, force pandoc to treat as markdown\n",
        "    input_format = \"markdown_strict\" if ext == \".txt\" else None\n",
        "    pypandoc.convert_file(\n",
        "        in_file,\n",
        "        to=format_to,\n",
        "        format=input_format,   # fix for txt\n",
        "        outputfile=out_file,\n",
        "        extra_args=['--standalone']\n",
        "    )\n",
        "    return out_file\n",
        "\n",
        "# ---------- FILE TRANSLATION ----------\n",
        "def translate_text(text, src_lang=\"auto\", dest_lang=\"en\"):\n",
        "    translated_chunks = []\n",
        "    chunk_size = 4000  # Google limit\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunk = text[i:i+chunk_size]\n",
        "        translated = GoogleTranslator(source=src_lang, target=dest_lang).translate(chunk)\n",
        "        translated_chunks.append(translated)\n",
        "    return \"\\n\".join(translated_chunks)\n",
        "\n",
        "def translate_file(file_path, out_path, src_lang=\"auto\", dest_lang=\"en\"):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Extract text\n",
        "    if ext == \".pdf\":\n",
        "        text_file = pdf_to_text(file_path, \"temp.txt\")\n",
        "        with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "    elif ext == \".docx\":\n",
        "        text_file = doc_to_text(file_path, \"temp.txt\")\n",
        "        with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "    # Translate (with chunking)\n",
        "    translated_text = translate_text(content, src_lang, dest_lang)\n",
        "\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(translated_text)\n",
        "\n",
        "    return out_path\n",
        "\n",
        "# ---------- CLI MENU ----------\n",
        "def main():\n",
        "    while True:\n",
        "        print(\"\\n===== File Toolkit =====\")\n",
        "        print(\"1. Convert File\")\n",
        "        print(\"2. Translate File\")\n",
        "        print(\"3. Exit\")\n",
        "        choice = input(\"Enter choice: \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            uploaded = files.upload()   # File upload dialog\n",
        "            in_file = list(uploaded.keys())[0]\n",
        "            print(\"\\nConvert To -> pdf, txt, docx, csv, xls, png (image)\")\n",
        "            fmt = input(\"Enter target format: \").strip().lower()\n",
        "            out_file = os.path.splitext(in_file)[0] + \"_converted.\" + fmt\n",
        "\n",
        "            try:\n",
        "                ext = os.path.splitext(in_file)[1].lower()\n",
        "                if ext == \".pdf\" and fmt == \"txt\":\n",
        "                    pdf_to_text(in_file, out_file)\n",
        "                elif ext == \".docx\" and fmt == \"txt\":\n",
        "                    doc_to_text(in_file, out_file)\n",
        "                elif ext == \".txt\" and fmt == \"png\":\n",
        "                    text_to_image(in_file, out_file)\n",
        "                elif ext == \".csv\" and fmt == \"xls\":\n",
        "                    csv_to_xls(in_file, out_file)\n",
        "                else:\n",
        "                    generic_convert(in_file, out_file, fmt)\n",
        "                print(f\"✅ Converted file saved at {out_file}\")\n",
        "            except Exception as e:\n",
        "                print(\"❌ Conversion failed:\", e)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            uploaded = files.upload()\n",
        "            in_file = list(uploaded.keys())[0]\n",
        "            src_lang = input(\"Source language code (or 'auto'): \").strip()\n",
        "            dest_lang = input(\"Target language code (e.g., en, hi, fr, de): \").strip()\n",
        "            out_file = os.path.splitext(in_file)[0] + f\"_{dest_lang}.txt\"\n",
        "            try:\n",
        "                translate_file(in_file, out_file, src_lang, dest_lang)\n",
        "                print(f\"✅ Translated file saved at {out_file}\")\n",
        "            except Exception as e:\n",
        "                print(\"❌ Translation failed:\", e)\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TdiKLeIenwu"
      },
      "source": [
        "### Download output file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX-N8dHJ70g3"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"B20442415 - Copy_hi.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdxE_LPGlZ9A"
      },
      "source": [
        "# Font download in colab :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AejVEcvhlBMJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url_regular = \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf\"\n",
        "url_devanagari = \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Medium.ttf\"\n",
        "output_regular = \"NotoSans-Regular.ttf\"\n",
        "output_devanagari = \"NotoSansDevanagari-Medium.ttf\"\n",
        "\n",
        "def download_font(url, output_path):\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    if r.status_code == 200:\n",
        "        with open(output_path, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "        print(f\"✅ Font downloaded: {output_path}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"❌ Download failed for {url}: {r.status_code}\")\n",
        "        return False\n",
        "\n",
        "download_font(url_regular, output_regular)\n",
        "download_font(url_devanagari, output_devanagari)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mLw5aAu5IHB"
      },
      "source": [
        "# Manual Font Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMJe3AgC5NW2"
      },
      "outputs": [],
      "source": [
        "# Browser open karo.\n",
        "\n",
        "#URL open karo:\n",
        "\n",
        "https://github.com/googlefonts/noto-fonts/blob/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf\n",
        "\n",
        "#Download raw file (button ya Download option hoga).\n",
        "\n",
        "#File ko apne project folder me NotoSans-Regular.ttf ke naam se save kar lo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAk6v4g2exEN"
      },
      "source": [
        "# txt_conversion.py Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DlbnTqTyN8vM"
      },
      "outputs": [],
      "source": [
        "# Upload a Unicode Devanagari font, e.g., NotoSansDevanagari-Regular.ttf\n",
        "\n",
        "# txt_converters.py\n",
        "# --- add at top of file (or keep existing imports) ---\n",
        "import os\n",
        "import html\n",
        "import shutil\n",
        "import os, csv, json, io, textwrap\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.utils import simpleSplit\n",
        "from reportlab.lib.pagesizes import A4, letter\n",
        "from reportlab.pdfgen import canvas as rl_canvas\n",
        "from reportlab.lib.utils import simpleSplit\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "from reportlab.pdfbase.cidfonts import UnicodeCIDFont\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "from docx.oxml import OxmlElement\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "from langdetect import detect\n",
        "\n",
        "# For Colab file handling\n",
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "    files = None\n",
        "\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "from reportlab.pdfbase.cidfonts import UnicodeCIDFont\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from langdetect import detect\n",
        "import regex  # better than re for Unicode script detection\n",
        "\n",
        "PAGE_W, PAGE_H = A4\n",
        "\n",
        "def detect_script(text):\n",
        "    \"\"\"Detect script of a line and return font name.\"\"\"\n",
        "    # Register multiple fonts\n",
        "    pdfmetrics.registerFont(TTFont(\"NotoSans\", \"/content/NotoSans-Regular.ttf\")) #Add Local path of font\n",
        "    pdfmetrics.registerFont(TTFont(\"NotoSansDevanagari\", \"/content/NotoSansDevanagari-Medium.ttf\")) #Add Local path of font\n",
        "\n",
        "    if regex.search(r'\\p{Devanagari}', text):\n",
        "        return \"NotoSansDevanagari\"\n",
        "    return \"NotoSans\"  # default English font\n",
        "\n",
        "def txt_to_pdf(txt_path, pdf_path, font_size=12, margin=40, line_gap=16):\n",
        "    # Register multiple fonts\n",
        "    pdfmetrics.registerFont(TTFont(\"NotoSans\", \"/content/NotoSans-Regular.ttf\")) #Add Local path of font\n",
        "    pdfmetrics.registerFont(TTFont(\"NotoSansDevanagari\", \"/content/NotoSansDevanagari-Medium.ttf\")) #Add Local path of font\n",
        "\n",
        "    try:\n",
        "        c = canvas.Canvas(pdf_path, pagesize=A4)\n",
        "\n",
        "        y = PAGE_H - margin\n",
        "        max_width = PAGE_W - 2 * margin\n",
        "\n",
        "        with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for raw_line in f:\n",
        "                line = raw_line.strip(\"\\n\")\n",
        "\n",
        "                # choose font dynamically\n",
        "                font_name = detect_script(line)\n",
        "                c.setFont(font_name, font_size)\n",
        "\n",
        "                wrapped_lines = simpleSplit(line, font_name, font_size, max_width)\n",
        "                if not wrapped_lines:\n",
        "                    wrapped_lines = [\" \"]\n",
        "\n",
        "                for wline in wrapped_lines:\n",
        "                    if y < margin:  # new page\n",
        "                        c.showPage()\n",
        "                        y = PAGE_H - margin\n",
        "                        c.setFont(font_name, font_size)\n",
        "\n",
        "                    c.drawString(margin, y, wline)\n",
        "                    y -= line_gap\n",
        "\n",
        "        c.save()\n",
        "        print(f\"✅ PDF saved: {pdf_path}\")\n",
        "        return pdf_path\n",
        "    except Exception as e:\n",
        "        print(\"❌ txt_to_pdf failed:\", str(e))\n",
        "        return None\n",
        "\n",
        "\n",
        "def txt_to_doc(input_file, output_file):\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # ✅ Detect language\n",
        "    try:\n",
        "        language = detect(text)\n",
        "    except:\n",
        "        language = \"en\"\n",
        "\n",
        "    # ✅ Font mapping by language\n",
        "    font_map = {\n",
        "        #\"hi\": \"Mangal\",        # Hindi\n",
        "        \"hi\": \"Nirmala UI\",     # Hindi\n",
        "        \"bn\": \"Vrinda\",           # Bengali\n",
        "        \"ta\": \"Latha\",            # Tamil\n",
        "        \"te\": \"Gautami\",          # Telugu\n",
        "        \"kn\": \"Tunga\",            # Kannada\n",
        "        \"ml\": \"Kartika\",          # Malayalam\n",
        "        \"gu\": \"Shruti\",           # Gujarati\n",
        "        \"pa\": \"Raavi\",            # Punjabi\n",
        "        \"or\": \"Kalinga\",          # Odia\n",
        "\n",
        "        \"zh-cn\": \"Microsoft YaHei\",   # Simplified Chinese\n",
        "        \"zh-tw\": \"PMingLiU\",          # Traditional Chinese\n",
        "        \"ja\": \"MS Mincho\",            # Japanese\n",
        "        \"ko\": \"Malgun Gothic\",        # Korean\n",
        "\n",
        "        \"ar\": \"Traditional Arabic\",   # Arabic\n",
        "        \"fa\": \"B Nazanin\",            # Persian/Farsi\n",
        "        \"ur\": \"Jameel Noori Nastaleeq\", # Urdu\n",
        "        \"he\": \"David\",                # Hebrew\n",
        "\n",
        "        \"en\": \"Arial\",         # English\n",
        "        \"ru\": \"Times New Roman\",      # Russian\n",
        "        \"uk\": \"Times New Roman\",      # Ukrainian\n",
        "        \"el\": \"Times New Roman\",      # Greek\n",
        "        \"th\": \"Angsana New\",          # Thai\n",
        "        \"vi\": \"Times New Roman\",      # Vietnamese\n",
        "\n",
        "        \"fr\": \"Calibri\",              # French\n",
        "        \"de\": \"Calibri\",              # German\n",
        "        \"es\": \"Calibri\",              # Spanish\n",
        "        \"it\": \"Calibri\",              # Italian\n",
        "        \"pt\": \"Calibri\",              # Portuguese\n",
        "    }\n",
        "    chosen_font = font_map.get(language, \"Noto Sans\") # fallback Noto Sans\n",
        "\n",
        "    doc = Document()\n",
        "    style = doc.styles['Normal']\n",
        "    style.font.name = chosen_font\n",
        "    style.font.size = Pt(12)\n",
        "\n",
        "    for line in text.splitlines():\n",
        "        if line.strip():  # skip blank lines\n",
        "                p = doc.add_paragraph(line.strip())\n",
        "                r = p.runs[0]\n",
        "                r.font.name = chosen_font\n",
        "                r._element.rPr.rFonts.set(qn('w:eastAsia'), chosen_font)\n",
        "\n",
        "    doc.save(output_file)\n",
        "    print(f\"✅ Saved {output_file} with font '{chosen_font}' (detected lang: {language})\")\n",
        "\n",
        "    # ✅ Suggestion for user\n",
        "    if language not in font_map:\n",
        "        print(f\"⚠️ Language '{language}' not mapped. \"\n",
        "              f\"Please try 'Noto Sans' or manually set an appropriate font in MS Word.\")\n",
        "    else:\n",
        "        print(f\"💡 Tip: If the text doesn’t render well, \"\n",
        "              f\"manually set the font in MS Word to '{chosen_font}' for proper display.\")\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import os\n",
        "from langdetect import detect\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import os\n",
        "from langdetect import detect\n",
        "\n",
        "#---------------- Font Detection ------------------\n",
        "def get_font_for_line(line, font_size):\n",
        "    # font mapping by language\n",
        "    FONT_MAP = {\n",
        "        \"hi\": \"/content/NotoSansDevanagari-Medium.ttf\",  # Hindi\n",
        "        \"en\": \"/content/NotoSans-Regular.ttf\",           # English\n",
        "        \"zh-cn\": \"/content/NotoSansSC-Regular.ttf\",      # Simplified Chinese\n",
        "        \"ja\": \"/content/NotoSansJP-Regular.ttf\",         # Japanese\n",
        "        \"ko\": \"/content/NotoSansKR-Regular.ttf\",         # Korean\n",
        "        \"default\": \"/content/NotoSans-Regular.ttf\"       # fallback\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        lang = detect(line) if line.strip() else \"en\"\n",
        "    except:\n",
        "        lang = \"en\"\n",
        "\n",
        "    # normalize\n",
        "    if lang.startswith(\"zh\"):\n",
        "        lang = \"zh-cn\"\n",
        "    elif lang.startswith(\"ja\"):\n",
        "        lang = \"ja\"\n",
        "    elif lang.startswith(\"ko\"):\n",
        "        lang = \"ko\"\n",
        "    elif lang.startswith(\"hi\"):\n",
        "        lang = \"hi\"\n",
        "    else:\n",
        "        lang = \"en\"\n",
        "\n",
        "    font_path = FONT_MAP.get(lang, FONT_MAP[\"default\"])\n",
        "    if not os.path.exists(font_path):  # fallback if missing\n",
        "        font_path = FONT_MAP[\"default\"]\n",
        "\n",
        "    return ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "\n",
        "#---------------- TXT to Image ------------------\n",
        "def txt_to_image(txt_path, output_dir, font_size=24, width=1240, height=1754,\n",
        "                 margin=40, max_lines_per_img=70, split=None):\n",
        "\n",
        "    try:\n",
        "        # Read text file\n",
        "        with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        images = []\n",
        "        line_height = font_size + 10\n",
        "\n",
        "        # Auto decide if split is None\n",
        "        if split is None:\n",
        "            if len(lines) <= 20:\n",
        "                split = False   # single image\n",
        "            else:\n",
        "                # user choice\n",
        "                choice = input(\"Text has more than 20 lines. Do you want multiple images? (y/n): \")\n",
        "                split = True if choice.lower().startswith(\"y\") else False\n",
        "\n",
        "        if not split:\n",
        "            # Option 1: All lines in ONE big image\n",
        "            total_height = margin*2 + len(lines)*line_height\n",
        "            img = Image.new(\"RGB\", (width, total_height), \"white\")\n",
        "            draw = ImageDraw.Draw(img)\n",
        "\n",
        "            y = margin\n",
        "            for line in lines:\n",
        "                font = get_font_for_line(line, font_size)\n",
        "                draw.text((margin, y), line.strip(), fill=\"black\", font=font)\n",
        "                y += line_height\n",
        "\n",
        "            out_path = os.path.join(output_dir, \"output_single.png\")\n",
        "            img.save(out_path)\n",
        "            images.append(out_path)\n",
        "\n",
        "        else:\n",
        "            # Option 2: Split into multiple images\n",
        "            chunks = [lines[i:i+max_lines_per_img] for i in range(0, len(lines), max_lines_per_img)]\n",
        "\n",
        "            for idx, chunk in enumerate(chunks, start=1):\n",
        "                img = Image.new(\"RGB\", (width, height), \"white\")\n",
        "                draw = ImageDraw.Draw(img)\n",
        "\n",
        "                y = margin\n",
        "                for line in chunk:\n",
        "                    font = get_font_for_line(line, font_size)\n",
        "                    draw.text((margin, y), line.strip(), fill=\"black\", font=font)\n",
        "                    y += line_height\n",
        "\n",
        "                out_path = os.path.join(output_dir, f\"page_{idx}.png\")\n",
        "                img.save(out_path)\n",
        "                images.append(out_path)\n",
        "\n",
        "        print(f\"✅ Images saved: {images}\")\n",
        "        return images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ txt_to_image failed:\", str(e))\n",
        "        return None\n",
        "\n",
        "import os, csv\n",
        "import sys\n",
        "\n",
        "# Optional libs\n",
        "try:\n",
        "    from langdetect import detect, detect_langs\n",
        "    _HAS_LANGDETECT = True\n",
        "except Exception:\n",
        "    _HAS_LANGDETECT = False\n",
        "\n",
        "# pandas only if user wants xlsx export or fallback viewing\n",
        "try:\n",
        "    import pandas as pd\n",
        "    _HAS_PANDAS = True\n",
        "except Exception:\n",
        "    _HAS_PANDAS = False\n",
        "\n",
        "# For local GUI popup (falls back to print)\n",
        "try:\n",
        "    import tkinter as tk\n",
        "    from tkinter import messagebox\n",
        "    _HAS_TK = True\n",
        "except Exception:\n",
        "    _HAS_TK = False\n",
        "\n",
        "def _auto_detect_delimiter(txt_path, sample_bytes=8192):\n",
        "    \"\"\"Try to detect delimiter from first KBs of file; fallback to tab.\"\"\"\n",
        "    try:\n",
        "        with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            sample = f.read(sample_bytes)\n",
        "        sniffer = csv.Sniffer()\n",
        "        dialect = sniffer.sniff(sample)\n",
        "        return dialect.delimiter\n",
        "    except Exception:\n",
        "        # Common fallbacks\n",
        "        for d in [\",\", \"\\t\", \";\", \"|\"]:\n",
        "            if d in sample:\n",
        "                return d\n",
        "    return \"\\t\"\n",
        "\n",
        "def _detect_non_english_and_scripts(txt_path, max_chars=10000):\n",
        "    \"\"\"\n",
        "    Returns tuple (is_non_english_bool, top_languages_list, has_non_ascii_bool, scripts_set)\n",
        "    Uses langdetect if available; always returns heuristic based on non-ASCII presence.\n",
        "    \"\"\"\n",
        "    text_sample = \"\"\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        # read some lines to keep it fast for very large files\n",
        "        for i, line in enumerate(f):\n",
        "            text_sample += line\n",
        "            if len(text_sample) >= max_chars:\n",
        "                break\n",
        "\n",
        "    has_non_ascii = any(ord(ch) > 127 for ch in text_sample)\n",
        "    top_langs = []\n",
        "    non_english = False\n",
        "\n",
        "    if _HAS_LANGDETECT:\n",
        "        try:\n",
        "            langs = detect_langs(text_sample)\n",
        "            top_langs = [(str(l).split(\":\")[0], float(str(l).split(\":\")[1])) for l in langs]\n",
        "            # consider non-english if top language not 'en' with decent prob\n",
        "            if top_langs:\n",
        "                top_lang = top_langs[0][0]\n",
        "                top_prob = top_langs[0][1]\n",
        "                non_english = not (top_lang == \"en\" and top_prob >= 0.65)\n",
        "        except Exception:\n",
        "            top_langs = []\n",
        "            non_english = has_non_ascii\n",
        "    else:\n",
        "        non_english = has_non_ascii\n",
        "\n",
        "    # simple script heuristics (return set like {'Devanagari','CJK','Arabic',...})\n",
        "    scripts = set()\n",
        "    for ch in text_sample[:2000]:\n",
        "        cp = ord(ch)\n",
        "        if 0x0900 <= cp <= 0x097F:\n",
        "            scripts.add(\"Devanagari\")\n",
        "        elif 0x4E00 <= cp <= 0x9FFF:\n",
        "            scripts.add(\"CJK\")\n",
        "        elif 0x3040 <= cp <= 0x30FF:\n",
        "            scripts.add(\"Japanese\")\n",
        "        elif 0xAC00 <= cp <= 0xD7AF:\n",
        "            scripts.add(\"Hangul\")\n",
        "        elif 0x0600 <= cp <= 0x06FF:\n",
        "            scripts.add(\"Arabic\")\n",
        "        elif cp > 127 and not ch.isspace():\n",
        "            scripts.add(\"OtherNonASCII\")\n",
        "\n",
        "    return non_english, top_langs, has_non_ascii, scripts\n",
        "\n",
        "def _notify_user_excel_instructions(csv_path, non_english, top_langs, scripts):\n",
        "    \"\"\"\n",
        "    Shows instructions to user on how to open CSV in Excel correctly.\n",
        "    If running in Colab -> just prints instructions. If local & tkinter available -> show popup.\n",
        "    \"\"\"\n",
        "    languages_text = \"\"\n",
        "    if top_langs:\n",
        "        languages_text = \", \".join([f\"{l}:{p:.2f}\" for l, p in top_langs])\n",
        "    else:\n",
        "        languages_text = \"detected non-ASCII text\" if non_english else \"English/ASCII\"\n",
        "\n",
        "    instructions = (\n",
        "        f\"CSV saved at: {csv_path}\\n\\n\"\n",
        "        f\"Detected languages/sample: {languages_text}\\n\\n\"\n",
        "        \"For best results open this CSV in Excel like this:\\n\"\n",
        "        \"1) Open Excel -> Data tab -> Get Data -> From Text/CSV\\n\"\n",
        "        \"2) Select the file and when the preview appears, choose 'File origin' = UTF-8 (or '65001: Unicode (UTF-8)')\\n\"\n",
        "        \"3) Click 'Load'.\\n\\n\"\n",
        "        \"Alternative (no import wizard): this script saved the file with a BOM (utf-8-sig) so modern Excel often opens it fine by double-clicking.\\n\\n\"\n",
        "        \"If users still see garbled characters, consider opening in LibreOffice or importing via the Data menu and explicitly choosing UTF-8.\\n\\n\"\n",
        "        \"Tip: To avoid Excel problems at all, open the provided .xlsx version (if available) which preserves texts in all languages.\"\n",
        "    )\n",
        "\n",
        "    # If Colab environment, just print\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        in_colab = True\n",
        "    except Exception:\n",
        "        in_colab = False\n",
        "\n",
        "    if in_colab or not _HAS_TK:\n",
        "        print(\"\\n\" + \"=\"*60 + \"\\nIMPORTANT: How to open this CSV in Excel\\n\" + \"=\"*60)\n",
        "        print(instructions)\n",
        "        return\n",
        "\n",
        "    # else show a tkinter popup (local desktop)\n",
        "    try:\n",
        "        root = tk.Tk()\n",
        "        root.withdraw()\n",
        "        # Use a scrolled text popup? Simpler: showinfo with basic instructions\n",
        "        messagebox.showinfo(\"CSV saved — How to open in Excel (UTF-8)\", instructions)\n",
        "        root.destroy()\n",
        "    except Exception:\n",
        "        print(instructions)\n",
        "\n",
        "\n",
        "def txt_to_csv(\n",
        "    txt_path,\n",
        "    csv_path,\n",
        "    delimiter=\"\\t\",\n",
        "    auto_detect_delimiter=False,\n",
        "    excel_friendly=True,   # writes utf-8-sig so Excel auto-detects BOM\n",
        "    notify_user=True,\n",
        "    save_xlsx=False        # if True and pandas available, also write .xlsx beside csv\n",
        "):\n",
        "    \"\"\"\n",
        "    Stream text -> CSV preserving UTF-8 and user friendly Excel behavior.\n",
        "\n",
        "    Parameters:\n",
        "      - txt_path: input text file path (UTF-8 expected)\n",
        "      - csv_path: output path (.csv)\n",
        "      - delimiter: delimiter to split lines into columns (default tab)\n",
        "      - auto_detect_delimiter: try to detect delimiter automatically from file sample\n",
        "      - excel_friendly: if True write using encoding='utf-8-sig' (BOM) -> Excel on Windows reads well\n",
        "      - notify_user: print or popup instructions on how to open in Excel if non-English content detected\n",
        "      - save_xlsx: if True and pandas installed, also write an .xlsx copy (recommended)\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(txt_path):\n",
        "        raise FileNotFoundError(f\"Input file not found: {txt_path}\")\n",
        "\n",
        "    # auto-detect delimiter if requested\n",
        "    if auto_detect_delimiter:\n",
        "        try:\n",
        "            delimiter = _auto_detect_delimiter(txt_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # detect non-english content & scripts\n",
        "    non_english, top_langs, has_non_ascii, scripts = _detect_non_english_and_scripts(txt_path)\n",
        "\n",
        "    # choose encoding\n",
        "    enc = \"utf-8-sig\" if excel_friendly else \"utf-8\"\n",
        "\n",
        "    # Write CSV streaming line-by-line\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as src, \\\n",
        "         open(csv_path, \"w\", encoding=enc, newline=\"\") as dst:\n",
        "        writer = csv.writer(dst)\n",
        "        for raw in src:\n",
        "            row = raw.rstrip(\"\\n\").split(delimiter)\n",
        "            # ensure row is list of str (no bytes)\n",
        "            row = [(\"\" if v is None else str(v)) for v in row]\n",
        "            writer.writerow(row)\n",
        "\n",
        "    # Optionally save XLSX to avoid Excel encoding issues completely\n",
        "    xlsx_path = None\n",
        "    if save_xlsx:\n",
        "        if not _HAS_PANDAS:\n",
        "            print(\"⚠️ pandas not installed; cannot save .xlsx. Install pandas + openpyxl to enable this.\")\n",
        "        else:\n",
        "            try:\n",
        "                # read via pandas (fast for reasonably sized files)\n",
        "                df = pd.read_csv(csv_path, encoding=enc)\n",
        "                xlsx_path = os.path.splitext(csv_path)[0] + \".xlsx\"\n",
        "                df.to_excel(xlsx_path, index=False, engine=\"openpyxl\")\n",
        "            except Exception as e:\n",
        "                print(\"⚠️ Could not create .xlsx:\", e)\n",
        "\n",
        "    # Notify user (print or popup) with instructions if non-english content or user asked to be notified\n",
        "    if notify_user:\n",
        "        _notify_user_excel_instructions(csv_path, non_english, top_langs, scripts)\n",
        "\n",
        "    return {\"csv\": csv_path, \"xlsx\": xlsx_path, \"non_english\": non_english, \"langs\": top_langs, \"scripts\": scripts}\n",
        "\n",
        "def txt_to_xls(txt_path, xls_path, delimiter=\"\\t\"):\n",
        "    wb = Workbook(write_only=True)\n",
        "    ws = wb.create_sheet(\"Sheet1\")\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as src:\n",
        "        for line in src:\n",
        "            ws.append(line.rstrip(\"\\n\").split(delimiter))\n",
        "    wb.save(xls_path)\n",
        "    return xls_path\n",
        "\n",
        "def txt_to_json(txt_path, json_path):\n",
        "    # Converts each non-empty line to list; if delimiter-like structure exists, user can post-process.\n",
        "    data = []\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if s:\n",
        "                data.append(s)\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as out:\n",
        "        json.dump(data, out, ensure_ascii=False, indent=2)\n",
        "    return json_path\n",
        "\n",
        "\n",
        "# ---------------- CLI MENU ---------------- #\n",
        "\n",
        "def main():\n",
        "    while True:\n",
        "        print(\"\\n==== TXT File Converter ====\")\n",
        "        print(\"1. Convert TXT File\")\n",
        "        print(\"2. Exit\")\n",
        "        choice = input(\"Enter choice: \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            # Upload file (Colab) or enter path (offline)\n",
        "            if files:\n",
        "                txt_file = input(\"Enter path of TXT file: \").strip()\n",
        "                #uploaded = files.upload()\n",
        "                #txt_file = list(uploaded.keys())[0]\n",
        "            else:\n",
        "                txt_file = input(\"Enter path of TXT file: \").strip()\n",
        "\n",
        "            print(\"\\nSupported conversions: \")\n",
        "            print(\"1. TXT → PDF\")\n",
        "            print(\"2. TXT → DOCX\")\n",
        "            print(\"3. TXT → PNG (image)\")\n",
        "            print(\"4. TXT → CSV\")\n",
        "            print(\"5. TXT → XLSX\")\n",
        "            print(\"6. TXT → JSON\")\n",
        "            fmt_choice = input(\"Select target format (1-6): \").strip()\n",
        "\n",
        "            base, _ = os.path.splitext(txt_file)\n",
        "            out_file = None\n",
        "\n",
        "            try:\n",
        "                if fmt_choice == \"1\":\n",
        "                    out_file = base + \".pdf\"\n",
        "                    txt_to_pdf(txt_file, out_file)\n",
        "                elif fmt_choice == \"2\":\n",
        "                    out_file = base + \".docx\"\n",
        "                    txt_to_doc(txt_file, out_file)\n",
        "                elif fmt_choice == \"3\":\n",
        "                    out_file = base + \".png\"\n",
        "                    txt_to_image(txt_file, out_file)\n",
        "                elif fmt_choice == \"4\":\n",
        "                    out_file = base + \".csv\"\n",
        "                    txt_to_csv(txt_file, out_file)\n",
        "                elif fmt_choice == \"5\":\n",
        "                    out_file = base + \".xlsx\"\n",
        "                    txt_to_xls(txt_file, out_file)\n",
        "                elif fmt_choice == \"6\":\n",
        "                    out_file = base + \".json\"\n",
        "                    txt_to_json(txt_file, out_file)\n",
        "                else:\n",
        "                    print(\"❌ Invalid choice!\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"✅ Converted successfully: {out_file}\")\n",
        "\n",
        "                # Colab download option\n",
        "                if files:\n",
        "                    files.download(out_file)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"❌ Conversion failed:\", e)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            print(\"👋 Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"❌ Invalid choice!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr-9rwy3sV3G"
      },
      "source": [
        "# Install poppler for PDF to Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c7eUSwNrsgRr"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y poppler-utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtwPABpdUbhM"
      },
      "source": [
        "# PDF Converter Script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vgCnK0zvUotg"
      },
      "outputs": [],
      "source": [
        "# pdf_converters.py\n",
        "import os, csv, json\n",
        "import pdfplumber\n",
        "from docx import Document\n",
        "from pdf2image import convert_from_path\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from docx.shared import Pt\n",
        "from docx.shared import Inches\n",
        "from docx.oxml.ns import qn\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import unicodedata\n",
        "from openpyxl.styles import Font\n",
        "\n",
        "def pdf_to_txt(pdf_path, txt_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf, open(txt_path, \"w\", encoding=\"utf-8\") as out:\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text() or \"\"\n",
        "            out.write(text.rstrip() + \"\\n\\n\")\n",
        "    return txt_path\n",
        "\n",
        "\n",
        "# ---- Script to Font mapping ----\n",
        "FALLBACK_FONTS = {\n",
        "    \"LATIN\": \"Times New Roman\",\n",
        "    \"DEVANAGARI\": \"Nirmala UI\",        # Hindi\n",
        "    \"CJK\": \"SimSun\",                   # Chinese/Japanese/Korean\n",
        "    \"ARABIC\": \"Amiri\",\n",
        "    \"GREEK\": \"Palatino Linotype\",\n",
        "    \"DEFAULT\": \"Arial\"\n",
        "}\n",
        "\n",
        "def detect_script(text):\n",
        "    \"\"\"Detects script of the text based on Unicode names\"\"\"\n",
        "    for ch in text:\n",
        "        name = unicodedata.name(ch, \"\")\n",
        "        if \"DEVANAGARI\" in name:\n",
        "            return \"DEVANAGARI\"\n",
        "        elif any(x in name for x in [\"CJK UNIFIED\", \"HIRAGANA\", \"KATAKANA\", \"HANGUL\"]):\n",
        "            return \"CJK\"\n",
        "        elif \"ARABIC\" in name:\n",
        "            return \"ARABIC\"\n",
        "        elif \"GREEK\" in name:\n",
        "            return \"GREEK\"\n",
        "    return \"LATIN\"\n",
        "\n",
        "def pdf_to_doc(pdf_path, docx_path):\n",
        "    try:\n",
        "        doc = Document()\n",
        "\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for pageno, page in enumerate(pdf.pages, 1):\n",
        "\n",
        "                # --- Extract Text ---\n",
        "                text = page.extract_text() or \"\"\n",
        "                for line in text.splitlines():\n",
        "                    script = detect_script(line)\n",
        "                    font_name = FALLBACK_FONTS.get(script, FALLBACK_FONTS[\"DEFAULT\"])\n",
        "\n",
        "                    para = doc.add_paragraph()\n",
        "                    run = para.add_run(line)\n",
        "                    run.font.name = font_name\n",
        "                    run.font.size = Pt(11)\n",
        "\n",
        "                    # set for East Asian / Complex Scripts\n",
        "                    rPr = run._element.rPr.rFonts\n",
        "                    rPr.set(qn('w:eastAsia'), font_name)\n",
        "                    rPr.set(qn('w:cs'), font_name)\n",
        "                    rPr.set(qn('w:hAnsi'), font_name)\n",
        "\n",
        "                # --- Extract Images ---\n",
        "                for img in page.images:\n",
        "                    try:\n",
        "                        x0, top, x1, bottom = img[\"x0\"], img[\"top\"], img[\"x1\"], img[\"bottom\"]\n",
        "                        cropped = page.crop((x0, top, x1, bottom))\n",
        "                        pil_img = cropped.to_image(resolution=150).original\n",
        "                        img_path = f\"temp_img_{pageno}.png\"\n",
        "                        pil_img.save(img_path)\n",
        "\n",
        "                        # insert into docx\n",
        "                        doc.add_picture(img_path, width=Inches(4))\n",
        "                    except Exception as e:\n",
        "                        print(f\"⚠️ Image on page {pageno} skipped: {e}\")\n",
        "\n",
        "                if pageno < len(pdf.pages):\n",
        "                    doc.add_page_break()\n",
        "\n",
        "        doc.save(docx_path)\n",
        "        return docx_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def pdf_to_image(pdf_path, out_dir, fmt=\"png\", dpi=150, base_name=\"page\"):\n",
        "    try:\n",
        "      os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "      # convert all pages\n",
        "      images = convert_from_path(pdf_path, dpi=dpi)\n",
        "      page_count = len(images)\n",
        "\n",
        "      if page_count == 1:\n",
        "          # single page → direct save\n",
        "          out_path = os.path.join(out_dir, f\"{base_name}_1.{fmt}\")\n",
        "          images[0].save(out_path, fmt.upper())\n",
        "          print(f\"✅ Single-page PDF converted: {out_path}\")\n",
        "          return out_dir\n",
        "\n",
        "      # multi-page case → ask user choice\n",
        "      print(\"PDF has\", page_count, \"pages.\")\n",
        "      print(\"Choose output option:\")\n",
        "      print(\"1 → Merge all pages into ONE image\")\n",
        "      print(\"2 → Save each page as separate image\")\n",
        "      choice = input(\"Select option (1/2): \").strip()\n",
        "\n",
        "      if choice == \"1\":\n",
        "          # merge vertically into single tall image\n",
        "          widths, heights = zip(*(img.size for img in images))\n",
        "          total_height = sum(heights)\n",
        "          max_width = max(widths)\n",
        "\n",
        "          merged_img = Image.new(\"RGB\", (max_width, total_height), (255, 255, 255))\n",
        "\n",
        "          y_offset = 0\n",
        "          for img in images:\n",
        "              merged_img.paste(img, (0, y_offset))\n",
        "              y_offset += img.height\n",
        "\n",
        "          out_path = os.path.join(out_dir, f\"{base_name}_merged.{fmt}\")\n",
        "          merged_img.save(out_path, fmt.upper())\n",
        "          print(f\"✅ All pages merged into one image: {out_path}\")\n",
        "\n",
        "      else:\n",
        "          # separate images\n",
        "          for i, img in enumerate(images, 1):\n",
        "              img.save(os.path.join(out_dir, f\"{base_name}_{i}.{fmt}\"), fmt.upper())\n",
        "          print(f\"✅ {page_count} pages saved as separate images in {out_dir}\")\n",
        "\n",
        "      return out_dir\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---------- Helpers: script detection & font mapping ----------\n",
        "def detect_script_simple(text):\n",
        "    \"\"\"\n",
        "    Heuristic script detection based on Unicode codepoints.\n",
        "    Returns one of: DEVANAGARI, CJK, ARABIC, GREEK, HANGUL, LATIN, OTHER\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"LATIN\"\n",
        "    for ch in text:\n",
        "        cp = ord(ch)\n",
        "        # Devanagari\n",
        "        if 0x0900 <= cp <= 0x097F:\n",
        "            return \"DEVANAGARI\"\n",
        "        # CJK unified ideographs (Chinese)\n",
        "        if 0x4E00 <= cp <= 0x9FFF or 0x3400 <= cp <= 0x4DBF:\n",
        "            return \"CJK\"\n",
        "        # Hiragana / Katakana / Kanji ranges also considered CJK/Japanese\n",
        "        if 0x3040 <= cp <= 0x30FF:\n",
        "            return \"CJK\"\n",
        "        # Hangul (Korean)\n",
        "        if 0xAC00 <= cp <= 0xD7AF:\n",
        "            return \"HANGUL\"\n",
        "        # Arabic\n",
        "        if 0x0600 <= cp <= 0x06FF or 0x0750 <= cp <= 0x077F:\n",
        "            return \"ARABIC\"\n",
        "        # Greek\n",
        "        if 0x0370 <= cp <= 0x03FF:\n",
        "            return \"GREEK\"\n",
        "        # Basic Latin (ASCII)\n",
        "        if cp <= 0x007F:\n",
        "            continue\n",
        "        # If many other non-ascii chars, mark OTHER as fallback\n",
        "    # fallback: if there are non-ascii chars but not matched above\n",
        "    if any(ord(ch) > 127 for ch in text):\n",
        "        return \"OTHER\"\n",
        "    return \"LATIN\"\n",
        "\n",
        "# Font mapping for XLSX cells (set to fonts commonly available on Windows; adjust if you prefer others)\n",
        "FONT_MAP = {\n",
        "    \"DEVANAGARI\": \"Nirmala UI\",         # good for Hindi on Windows\n",
        "    \"CJK\": \"Microsoft YaHei\",           # Simplified Chinese / good on Windows\n",
        "    \"HANGUL\": \"Malgun Gothic\",          # Korean on Windows\n",
        "    \"ARABIC\": \"Scheherazade\",           # try Arabic-friendly; fallback may occur\n",
        "    \"GREEK\": \"Palatino Linotype\",\n",
        "    \"OTHER\": \"Arial Unicode MS\",        # broad coverage (if present)\n",
        "    \"LATIN\": \"Arial\"\n",
        "}\n",
        "\n",
        "def _page_tables_to_rows(page):\n",
        "    \"\"\"\n",
        "    Try to extract tables from a pdfplumber page as rows.\n",
        "    If no tables detected, fall back to extracting text lines and splitting heuristically.\n",
        "    Returns a list of row-lists.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    # 1) Try extract_tables (pdfplumber)\n",
        "    try:\n",
        "        tables = page.extract_tables()  # returns list of tables, each table is list of rows\n",
        "        if tables:\n",
        "            for table in tables:\n",
        "                # table: list of rows (cells may be None)\n",
        "                for row in table:\n",
        "                    rows.append([cell for cell in row])\n",
        "            if rows:\n",
        "                return rows\n",
        "    except Exception:\n",
        "        # continue to fallback\n",
        "        pass\n",
        "\n",
        "    # 2) Fallback: use page.extract_text() and split lines by common delimiters (tab or comma)\n",
        "    text = page.extract_text() or \"\"\n",
        "    if not text:\n",
        "        return []  # nothing to do\n",
        "    # attempt to detect delimiter by checking first non-empty line\n",
        "    lines = [ln for ln in text.splitlines() if ln.strip()]\n",
        "    if not lines:\n",
        "        return []\n",
        "    first = lines[0]\n",
        "    # choose delimiter heuristically\n",
        "    delim = \"\\t\"\n",
        "    if \",\" in first and first.count(\",\") >= 1:\n",
        "        delim = \",\"\n",
        "    elif \"|\" in first and first.count(\"|\") >= 1:\n",
        "        delim = \"|\"\n",
        "    # split each line\n",
        "    for ln in lines:\n",
        "        parts = [p.strip() for p in ln.split(delim)]\n",
        "        rows.append(parts)\n",
        "    return rows\n",
        "\n",
        "# ---------- Main function: writes CSV (streaming) and XLSX (write_only) ----------\n",
        "def pdf_to_csv(pdf_path, csv_path=None, xlsx_path=None,\n",
        "                      csv_delimiter=\",\", excel_font_map=FONT_MAP,\n",
        "                      batch_log_every=1000):\n",
        "    \"\"\"\n",
        "    PDF → CSV/XLSX converter (strict mode):\n",
        "    - Converts only actual tables\n",
        "    - Ignores all other content (JSON/plain text/XLS)\n",
        "    - Exits if no tables found\n",
        "    \"\"\"\n",
        "    if not csv_path and not xlsx_path:\n",
        "        raise ValueError(\"Provide at least one of csv_path or xlsx_path\")\n",
        "\n",
        "    csv_file = None\n",
        "    csv_writer = None\n",
        "    if csv_path:\n",
        "        csv_file = open(csv_path, \"w\", encoding=\"utf-8-sig\", newline=\"\")\n",
        "        csv_writer = csv.writer(csv_file, delimiter=csv_delimiter)\n",
        "\n",
        "    wb = None\n",
        "    ws = None\n",
        "    if xlsx_path:\n",
        "        wb = Workbook(write_only=True)\n",
        "        ws = wb.create_sheet(title=\"Extracted\")\n",
        "\n",
        "    total_rows = 0\n",
        "    unsupported_pages = []\n",
        "    valid_table_found = False\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            num_pages = len(pdf.pages)\n",
        "            print(f\"Opened PDF: {pdf_path} (pages: {num_pages})\")\n",
        "\n",
        "            for pageno, page in enumerate(pdf.pages, start=1):\n",
        "                rows = _page_tables_to_rows(page)\n",
        "\n",
        "                # Strict table check: must have >1 row and >1 column\n",
        "                if not rows or len(rows) < 1 or all(len(r) < 2 for r in rows):\n",
        "                    unsupported_pages.append(pageno)\n",
        "                    continue\n",
        "\n",
        "                # Valid table found\n",
        "                valid_table_found = True\n",
        "\n",
        "                # Write rows\n",
        "                for row in rows:\n",
        "                    out_row = [(\"\" if c is None else str(c)) for c in row]\n",
        "\n",
        "                    if csv_writer:\n",
        "                        try:\n",
        "                            csv_writer.writerow(out_row)\n",
        "                        except Exception:\n",
        "                            safe_row = [s.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\") for s in out_row]\n",
        "                            csv_writer.writerow(safe_row)\n",
        "\n",
        "                    if ws:\n",
        "                        cells = []\n",
        "                        for cell_val in out_row:\n",
        "                            script = detect_script_simple(cell_val)\n",
        "                            font_name = excel_font_map.get(script, excel_font_map.get(\"DEFAULT\", \"Arial\"))\n",
        "                            cell = WriteOnlyCell(ws, value=cell_val)\n",
        "                            cell.font = Font(name=font_name, size=11)\n",
        "                            cells.append(cell)\n",
        "                        ws.append(cells)\n",
        "\n",
        "                    total_rows += 1\n",
        "                    if total_rows % batch_log_every == 0:\n",
        "                        print(f\"Processed rows: {total_rows} (page {pageno}/{num_pages})\")\n",
        "\n",
        "        if not valid_table_found:\n",
        "            # Close any opened files\n",
        "            if csv_file:\n",
        "                csv_file.close()\n",
        "                os.remove(csv_path)  # remove empty CSV\n",
        "            if wb:\n",
        "                os.remove(xlsx_path)\n",
        "            print(\"❌ No embedded table/CSV found in PDF. Only actual tables can be converted in strict mode.\")\n",
        "            return None\n",
        "\n",
        "        # Close/save files\n",
        "        if csv_file:\n",
        "            csv_file.close()\n",
        "        if wb:\n",
        "            wb.save(xlsx_path)\n",
        "\n",
        "        if unsupported_pages:\n",
        "            print(f\"⚠️ Pages skipped (unsupported content): {unsupported_pages}\")\n",
        "            print(\"Only actual tables converted. JSON/XLS/plain text ignored.\")\n",
        "\n",
        "        print(f\"✅ Done. Rows written: {total_rows}\")\n",
        "        out = {}\n",
        "        if csv_path:\n",
        "            out[\"csv\"] = os.path.abspath(csv_path)\n",
        "        if xlsx_path:\n",
        "            out[\"xlsx\"] = os.path.abspath(xlsx_path)\n",
        "        out[\"rows\"] = total_rows\n",
        "        return out\n",
        "\n",
        "    except Exception as exc:\n",
        "        try:\n",
        "            if csv_file and not csv_file.closed:\n",
        "                csv_file.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            if wb:\n",
        "                wb.save(xlsx_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "        raise RuntimeError(f\"Error converting PDF to CSV/XLSX: {exc}\") from exc\n",
        "\n",
        "\n",
        "\n",
        "def pdf_to_xls(pdf_path, xls_path):\n",
        "    wb = Workbook(write_only=True)\n",
        "    ws = wb.create_sheet(\"Sheet1\")\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            rows = _page_tables_to_rows(page)\n",
        "            for r in rows:\n",
        "                ws.append([str(x) if x is not None else \"\" for x in r])\n",
        "    wb.save(xls_path)\n",
        "    return xls_path\n",
        "\n",
        "def pdf_to_json(pdf_path, json_path):\n",
        "    pages_data = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for idx, page in enumerate(pdf.pages, 1):\n",
        "            page_dict = {\"page\": idx, \"text\": page.extract_text() or \"\"}\n",
        "            # Try tables too\n",
        "            tables = page.extract_tables() or []\n",
        "            if tables:\n",
        "                page_dict[\"tables\"] = tables\n",
        "            pages_data.append(page_dict)\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as out:\n",
        "        json.dump(pages_data, out, ensure_ascii=False, indent=2)\n",
        "    return json_path\n",
        "\n",
        "# ---------------- CLI MENU ---------------- #\n",
        "\n",
        "def main():\n",
        "    # For Colab file handling\n",
        "    try:\n",
        "        from google.colab import files\n",
        "    except ImportError:\n",
        "        files = None\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n==== PDF File Converter =====\")\n",
        "        print(\"1. Convert PDF File\")\n",
        "        print(\"2. Exit\")\n",
        "        choice = input(\"Enter choice: \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            # Upload file (Colab) or enter path (offline)\n",
        "            if files:\n",
        "                #uploaded = files.upload()\n",
        "                #pdf_file = list(uploaded.keys())[0]\n",
        "                pdf_file = input(\"Enter path of PDF file: \").strip()\n",
        "            else:\n",
        "                pdf_file = input(\"Enter path of PDF file: \").strip()\n",
        "\n",
        "            print(\"\\nSupported conversions: \")\n",
        "            print(\"1. PDF → TXT\")\n",
        "            print(\"2. PDF → DOCX\")\n",
        "            print(\"3. PDF → PNG (image)\")\n",
        "            print(\"4. PDF → CSV\")\n",
        "            print(\"5. PDF → XLSX\")\n",
        "            print(\"6. PDF → JSON\")\n",
        "            fmt_choice = input(\"Select target format (1-6): \").strip()\n",
        "\n",
        "            base, _ = os.path.splitext(pdf_file)\n",
        "            out_file = None\n",
        "\n",
        "            try:\n",
        "                if fmt_choice == \"1\":\n",
        "                    out_file = base + \".txt\"\n",
        "                    pdf_to_txt(pdf_file, out_file)\n",
        "                elif fmt_choice == \"2\":\n",
        "                    out_file = base + \".docx\"\n",
        "                    pdf_to_doc(pdf_file, out_file)\n",
        "                elif fmt_choice == \"3\":\n",
        "                    out_file = base + \".png\"\n",
        "                    out_dir = base + \"_images\"\n",
        "                    pdf_to_image(pdf_file, out_dir)\n",
        "                    out_file = out_dir # set out_file to directory for download\n",
        "                elif fmt_choice == \"4\":\n",
        "                    out_file = base + \".csv\"\n",
        "                    pdf_to_csv(pdf_file, out_file)\n",
        "                elif fmt_choice == \"5\":\n",
        "                    out_file = base + \".xlsx\"\n",
        "                    pdf_to_xls(pdf_file, out_file)\n",
        "                elif fmt_choice == \"6\":\n",
        "                    out_file = base + \".json\"\n",
        "                    pdf_to_json(pdf_file, out_file)\n",
        "                else:\n",
        "                    print(\"❌ Invalid choice!\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"✅ Converted successfully: {out_file}\")\n",
        "\n",
        "                # Colab download option\n",
        "                if files and out_file:\n",
        "                    if os.path.isdir(out_file): # if output is a directory (images)\n",
        "                         print(f\"💡 Multiple files saved in {out_file}. You may need to zip and download manually.\")\n",
        "                         # Example of zipping and offering download (requires zip installed)\n",
        "                         # !zip -r {out_file}.zip {out_file}\n",
        "                         # files.download(f\"{out_file}.zip\")\n",
        "                    else:\n",
        "                         files.download(out_file)\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"❌ Conversion failed:\", e)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            print(\"👋 Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"❌ Invalid choice!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f14OZntcZJrz"
      },
      "source": [
        "# Install Libraoffice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMJk2fijydvq"
      },
      "outputs": [],
      "source": [
        "  !sudo apt-get install -y libreoffice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4OP2ktc1g0t"
      },
      "source": [
        "# Doc to HTML Convert Supporting Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMA72Hyj1ptI"
      },
      "outputs": [],
      "source": [
        "!pip install mammoth weasyprint tdqm ijson\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a2ND6E0ZYK-"
      },
      "source": [
        "# Doc Converter Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMWveBQwZfbe"
      },
      "outputs": [],
      "source": [
        "# doc_converters.py\n",
        "import os, csv, json, zipfile, tempfile # Import tempfile here as well\n",
        "from docx import Document\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.utils import simpleSplit\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from PIL import Image, ImageDraw\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Preformatted, Table, TableStyle, Image as RLImage\n",
        "from reportlab.platypus import Spacer\n",
        "from pdf2image import convert_from_path # Corrected import\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from PyPDF2 import PdfMerger # Removed convert_from_path from this import\n",
        "\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.platypus import Paragraph\n",
        "from reportlab.lib.units import cm\n",
        "from reportlab.lib.utils import simpleSplit\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "from docx import Document as DocxReader\n",
        "from docx.shared import Inches\n",
        "import unicodedata # Import unicodedata for script detection\n",
        "import shutil\n",
        "import subprocess\n",
        "from typing import List, Optional\n",
        "import platform\n",
        "import pathlib\n",
        "import stat, requests, glob\n",
        "import mammoth\n",
        "from pdf2image import convert_from_path\n",
        "import pdfkit\n",
        "import json, re\n",
        "import os\n",
        "import threading\n",
        "\n",
        "# ---------- Page setup ----------\n",
        "PAGE_W, PAGE_H = A4\n",
        "\n",
        "# ---------------- Delete after 5 minutes ---------------- #\n",
        "def schedule_delete(file_path, delay=300):  # 300 sec = 5 minutes\n",
        "    def delete_file():\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                if os.path.isdir(file_path):\n",
        "                    import shutil\n",
        "                    shutil.rmtree(file_path)\n",
        "                else:\n",
        "                    os.remove(file_path)\n",
        "                print(f\"🗑 Deleted: {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to delete {file_path}: {e}\")\n",
        "    timer = threading.Timer(delay, delete_file)\n",
        "    timer.start()\n",
        "\n",
        "def doc_to_txt(docx_path, txt_path):\n",
        "    try:\n",
        "      doc = Document(docx_path)\n",
        "      with open(txt_path, \"w\", encoding=\"utf-8\") as out:\n",
        "          # Paragraphs\n",
        "          for para in doc.paragraphs:\n",
        "              out.write((para.text or \"\") + \"\\n\")\n",
        "          # Tables (append as TSV)\n",
        "          for table in doc.tables:\n",
        "              for row in table.rows:\n",
        "                  cells = [c.text.replace(\"\\n\",\" \").strip() for c in row.cells]\n",
        "                  out.write(\"\\t\".join(cells) + \"\\n\")\n",
        "      return txt_path\n",
        "    except Exception as e:\n",
        "      raise RuntimeError(f\"Error converting DOCX to TXT: {e}\") from e\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def _which(cmd: str) -> Optional[str]:\n",
        "    return shutil.which(cmd)\n",
        "\n",
        "def _ensure_dir(path: str):\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Script detection (used for font suggestions)\n",
        "# -------------------------\n",
        "def detect_script_simple(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"LATIN\"\n",
        "    for ch in text:\n",
        "        cp = ord(ch)\n",
        "        if 0x0900 <= cp <= 0x097F:  # Devanagari (Hindi, etc)\n",
        "            return \"DEVANAGARI\"\n",
        "        if (0x4E00 <= cp <= 0x9FFF) or (0x3400 <= cp <= 0x4DBF) or (0x3040 <= cp <= 0x30FF):\n",
        "            return \"CJK\"\n",
        "        if 0xAC00 <= cp <= 0xD7AF:\n",
        "            return \"HANGUL\"\n",
        "        if 0x0600 <= cp <= 0x06FF or 0x0750 <= cp <= 0x077F:\n",
        "            return \"ARABIC\"\n",
        "        if 0x0370 <= cp <= 0x03FF:\n",
        "            return \"GREEK\"\n",
        "    if any(ord(ch) > 127 for ch in text):\n",
        "        return \"OTHER\"\n",
        "    return \"LATIN\"\n",
        "\n",
        "# ---------------- Helpers: Fonts -----------------\n",
        "FONTS_DIR = \"fonts\"\n",
        "\n",
        "# -------------------------\n",
        "# Convert .doc -> .docx\n",
        "# -------------------------\n",
        "def convert_doc_to_docx_if_needed(input_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Converts .doc to .docx using LibreOffice (headless). Returns a path to a .docx file.\n",
        "    Raises RuntimeError if soffice is missing or conversion fails.\n",
        "    \"\"\"\n",
        "    if input_path.lower().endswith(\".docx\"):\n",
        "        return input_path\n",
        "\n",
        "    if not input_path.lower().endswith(\".doc\"):\n",
        "        raise RuntimeError(\"Unsupported file (expecting .doc/.docx): \" + input_path)\n",
        "\n",
        "    soffice = _which(\"soffice\")\n",
        "    if not soffice:\n",
        "        raise RuntimeError(\n",
        "            \"LibreOffice (soffice) not found. Install it to support .doc -> .docx conversion.\\n\"\n",
        "            \"Linux/Colab:  sudo apt-get update && sudo apt-get install -y libreoffice\"\n",
        "        )\n",
        "\n",
        "    tmp_out = tempfile.mkdtemp(prefix=\"doc2docx_\")\n",
        "    cmd = [soffice, \"--headless\", \"--convert-to\", \"docx\", \"--outdir\", tmp_out, input_path]\n",
        "    subprocess.check_call(cmd)\n",
        "    out = os.path.join(tmp_out, os.path.splitext(os.path.basename(input_path))[0] + \".docx\")\n",
        "    if not os.path.exists(out):\n",
        "        raise RuntimeError(\"LibreOffice did not produce DOCX.\")\n",
        "    return out\n",
        "\n",
        "# -------------------------\n",
        "# Attempt to install Noto fonts (best-effort for Linux/Colab)\n",
        "# -------------------------\n",
        "def ensure_noto_fonts_for_scripts(scripts: set):\n",
        "    \"\"\"\n",
        "    Best-effort: download & install a couple of Noto fonts into user fonts dir on Linux.\n",
        "    Only does anything on Linux-like systems (including Colab).\n",
        "    For CJK (Chinese/Japanese/Korean) we print instructions because those packages are large.\n",
        "    \"\"\"\n",
        "    if not scripts:\n",
        "        return\n",
        "\n",
        "    sysplat = platform.system().lower()\n",
        "    if sysplat not in (\"linux\", \"darwin\"):\n",
        "        # macOS/Windows: can't auto install reliably; give instructions\n",
        "        print(\"Note: automatic font installation only supported for Linux/Colab in this script.\")\n",
        "        print(\"If you need CJK or other fonts installed on your system, please install Noto fonts manually.\")\n",
        "        return\n",
        "\n",
        "    fonts_dir = os.path.expanduser(\"~/.local/share/fonts\")\n",
        "    os.makedirs(fonts_dir, exist_ok=True)\n",
        "\n",
        "    # URLs for fonts (GoogleFonts repo raw)\n",
        "    FONT_URLS = {\n",
        "        \"CJK\":        \"https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/SimplifiedChinese/NotoSansSC-Regular.otf\",\n",
        "        \"HANGUL\":     \"https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Korean/NotoSansKR-Regular.otf\",\n",
        "        \"ARABIC\":     \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoNaskhArabic/NotoNaskhArabic-Regular.ttf\",\n",
        "        \"GREEK\":      \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf\",\n",
        "        \"OTHER\":      \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf\",\n",
        "        \"LATIN\": \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf\",\n",
        "        \"DEVANAGARI\": \"https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansDevanagari/NotoSansDevanagari-Regular.ttf\",\n",
        "        # CJK heavy: recommend apt-get fonts-noto-cjk instead of downloading single files\n",
        "    }\n",
        "\n",
        "    downloaded = []\n",
        "    for s in scripts:\n",
        "        if s == \"CJK\":\n",
        "            # prefer apt package\n",
        "            print(\"CJK script detected. For best results install Noto CJK fonts on your system.\")\n",
        "            print(\"On Debian/Ubuntu you can run:\")\n",
        "            print(\"  sudo apt-get update && sudo apt-get install -y fonts-noto-cjk\")\n",
        "            continue\n",
        "        url = FONT_URLS.get(s, FONT_URLS.get(\"LATIN\"))\n",
        "        if not url:\n",
        "            continue\n",
        "        try:\n",
        "            fname = os.path.basename(url)\n",
        "            out_path = os.path.join(fonts_dir, fname)\n",
        "            if os.path.exists(out_path):\n",
        "                print(f\"Font already present: {out_path}\")\n",
        "                downloaded.append(out_path)\n",
        "                continue\n",
        "            import urllib.request\n",
        "            print(\"Downloading font:\", url)\n",
        "            urllib.request.urlretrieve(url, out_path)\n",
        "            os.chmod(out_path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)\n",
        "            downloaded.append(out_path)\n",
        "            print(\" → saved to\", out_path)\n",
        "        except Exception as e:\n",
        "            print(\"Failed to download font:\", e)\n",
        "\n",
        "    # refresh font cache (Linux)\n",
        "    try:\n",
        "        print(\"Refreshing font cache...\")\n",
        "        run([\"fc-cache\", \"-f\", \"-v\"])\n",
        "    except Exception as e:\n",
        "        print(\"fc-cache failed (non-fatal):\", e)\n",
        "\n",
        "    if downloaded:\n",
        "        print(\"Fonts installed (user-level). Restart your PDF viewer if needed.\")\n",
        "\n",
        "# -------------------------\n",
        "# Convert .docx -> .pdf using LibreOffice (preferred)\n",
        "# -------------------------\n",
        "def convert_docx_to_pdf_libreoffice(docx_path: str, output_pdf: str) -> str:\n",
        "    soffice = _which(\"soffice\")\n",
        "    if not soffice:\n",
        "        raise RuntimeError(\n",
        "            \"LibreOffice (soffice) not found. Install it for high-fidelity DOCX->PDF.\\n\"\n",
        "            \"Linux/Colab:  sudo apt-get update && sudo apt-get install -y libreoffice\"\n",
        "        )\n",
        "    outdir = os.path.dirname(os.path.abspath(output_pdf)) or \".\"\n",
        "    _ensure_dir(output_pdf)\n",
        "    cmd = [soffice, \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", outdir, docx_path]\n",
        "    subprocess.check_call(cmd)\n",
        "    produced = os.path.join(outdir, os.path.splitext(os.path.basename(docx_path))[0] + \".pdf\")\n",
        "    if not os.path.exists(produced):\n",
        "        raise RuntimeError(\"LibreOffice did not produce PDF.\")\n",
        "    if os.path.abspath(produced) != os.path.abspath(output_pdf):\n",
        "        os.replace(produced, output_pdf)\n",
        "    return output_pdf\n",
        "\n",
        "# ---------- Rasterize (guaranteed readability) ----------\n",
        "def rasterize_pdf_to_pdf(input_pdf: str, output_pdf: str, dpi: int = 200) -> str:\n",
        "    \"\"\"\n",
        "    Converts every page of PDF to an image, then merges them back into PDF.\n",
        "    Ensures fonts, multilingual text, tables, JSON, etc. are preserved visually.\n",
        "    \"\"\"\n",
        "    _ensure_dir(output_pdf)\n",
        "    images = convert_from_path(input_pdf, dpi=dpi)\n",
        "    if not images:\n",
        "        raise RuntimeError(\"No pages produced while rasterizing.\")\n",
        "\n",
        "    pdf_pages = []\n",
        "    for img in images:\n",
        "        if img.mode != \"RGB\":\n",
        "            img = img.convert(\"RGB\")\n",
        "        pdf_pages.append(img)\n",
        "\n",
        "    first, rest = pdf_pages[0], pdf_pages[1:]\n",
        "    first.save(output_pdf, save_all=True, append_images=rest)\n",
        "    return output_pdf\n",
        "\n",
        "\n",
        "# ---------- Main unified function ----------\n",
        "def doc_to_pdf(input_path: str,\n",
        "               output_pdf: str,\n",
        "               mode: str = \"auto\",\n",
        "               raster_dpi: int = 200) -> str:\n",
        "    \"\"\"\n",
        "    Convert .doc/.docx → .pdf\n",
        "\n",
        "    - mode=\"auto\":\n",
        "        * If doc has table(s) with >=5 columns → convert via HTML to PDF (avoids cropping).\n",
        "        * Else → direct via LibreOffice (fastest, keeps structure).\n",
        "    - mode=\"image\": LibreOffice → PDF → rasterized (pixel perfect, no font issues).\n",
        "    \"\"\"\n",
        "    if not os.path.exists(input_path):\n",
        "        raise RuntimeError(\"Input file not found: \" + input_path)\n",
        "\n",
        "    _ensure_dir(output_pdf)\n",
        "\n",
        "    # 1) Ensure DOCX\n",
        "    docx_path = convert_doc_to_docx_if_needed(input_path)\n",
        "\n",
        "    # --- Check if doc has wide tables (>=5 columns) ---\n",
        "    has_wide_tables = False\n",
        "    try:\n",
        "        from docx import Document\n",
        "        doc = Document(docx_path)\n",
        "        for tbl in doc.tables:\n",
        "            if tbl.rows and len(tbl.rows[0].cells) >= 5:\n",
        "                has_wide_tables = True\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not inspect tables in DOCX: {e}\")\n",
        "\n",
        "    # --- Wide tables? Use HTML → PDF ---\n",
        "    if has_wide_tables and mode == \"auto\":\n",
        "        try:\n",
        "\n",
        "            with open(docx_path, \"rb\") as f:\n",
        "                html = mammoth.convert_to_html(f).value\n",
        "            tmp_html = os.path.join(tempfile.gettempdir(), \"doc_tmp.html\")\n",
        "            with open(tmp_html, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(html)\n",
        "            pdfkit.from_file(tmp_html, output_pdf)\n",
        "            print(f\"✅ Converted via HTML path (wide table support): {output_pdf}\")\n",
        "            return output_pdf\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ HTML conversion failed, falling back to LibreOffice: {e}\")\n",
        "\n",
        "    # --- Default: DOCX → PDF via LibreOffice ---\n",
        "    tmp_dir = tempfile.mkdtemp(prefix=\"docx2pdf_\")\n",
        "    lo_pdf = os.path.join(tmp_dir, \"lo.pdf\")\n",
        "    try:\n",
        "        lo_pdf = convert_docx_to_pdf_libreoffice(docx_path, lo_pdf)\n",
        "    except Exception as e:\n",
        "        shutil.rmtree(tmp_dir)\n",
        "        raise RuntimeError(f\"LibreOffice conversion failed: {e}\")\n",
        "\n",
        "    if mode == \"auto\":\n",
        "        shutil.copy(lo_pdf, output_pdf)\n",
        "        shutil.rmtree(tmp_dir)\n",
        "        return output_pdf\n",
        "\n",
        "    if mode == \"image\":\n",
        "        raster_pdf = os.path.join(tmp_dir, \"raster.pdf\")\n",
        "        rasterize_pdf_to_pdf(lo_pdf, raster_pdf, dpi=raster_dpi)\n",
        "        shutil.copy(raster_pdf, output_pdf)\n",
        "        shutil.rmtree(tmp_dir)\n",
        "        return output_pdf\n",
        "\n",
        "    shutil.rmtree(tmp_dir)\n",
        "    raise ValueError('mode must be \"auto\" or \"image\"')\n",
        "\n",
        "\n",
        "#--------- Docx For Image---------------------\n",
        "def doc_to_docx_image(input_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Ensure the input is in DOCX format.\n",
        "    If it's a .doc, convert using LibreOffice headless.\n",
        "    \"\"\"\n",
        "    import tempfile, subprocess, os, glob\n",
        "\n",
        "    if input_path.lower().endswith(\".docx\"):\n",
        "        return input_path\n",
        "\n",
        "    if input_path.lower().endswith(\".doc\"):\n",
        "        tmp_dir = tempfile.mkdtemp(prefix=\"doc2docx_\")\n",
        "        try:\n",
        "            subprocess.check_call([\n",
        "                \"soffice\", \"--headless\", \"--convert-to\", \"docx\",\n",
        "                \"--outdir\", tmp_dir, input_path\n",
        "            ])\n",
        "\n",
        "            # ✅ Find the .docx file inside tmp_dir\n",
        "            converted_files = glob.glob(os.path.join(tmp_dir, \"*.docx\"))\n",
        "            if not converted_files:\n",
        "                raise RuntimeError(\"DOC→DOCX conversion failed, file not created.\")\n",
        "\n",
        "            return converted_files[0]  # Return the actual converted path\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to convert .doc to .docx: {e}\")\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unsupported file type: {input_path}\")\n",
        "\n",
        "def doc_to_image(input_path, out_path, dpi=200):\n",
        "    \"\"\"\n",
        "    Convert .doc/.docx → PNG image(s) with full formatting (tables, images, layout preserved).\n",
        "    - If 1 page → single PNG\n",
        "    - If multiple pages → user chooses:\n",
        "        1 = merge into single long PNG\n",
        "        2 = separate PNG per page (zipped)\n",
        "    \"\"\"\n",
        "    tmp_dir = None\n",
        "    try:\n",
        "        # ✅ Always ensure DOCX\n",
        "        docx_path = doc_to_docx_image(input_path)\n",
        "        # ✅ Detect wide tables\n",
        "        wide_table = False\n",
        "        try:\n",
        "            doc = Document(docx_path)\n",
        "            for table in doc.tables:\n",
        "                if len(table.columns) > 5:\n",
        "                    wide_table = True\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not inspect tables: {e}\")\n",
        "\n",
        "        # ========== PATH 1: Wide Table → Mammoth + WeasyPrint ==========\n",
        "        if wide_table:\n",
        "            print(\"⚠️ Wide table detected → Using HTML → PDF → Image pipeline\")\n",
        "            tmp_dir = tempfile.mkdtemp(prefix=\"doc2html_\")\n",
        "\n",
        "            # DOCX → HTML\n",
        "            with open(docx_path, \"rb\") as f:\n",
        "                result = mammoth.convert_to_html(f)\n",
        "                html_content = result.value\n",
        "\n",
        "            # Save HTML to file (safe for large tables)\n",
        "            html_file = os.path.join(tmp_dir, \"doc.html\")\n",
        "            with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(html_content)\n",
        "\n",
        "            # HTML → PDF\n",
        "            pdf_path = os.path.join(tmp_dir, \"out.pdf\")\n",
        "            pdfkit.from_file(html_file, pdf_path)\n",
        "\n",
        "        # ========== PATH 2: Normal → LibreOffice ==========\n",
        "        else:\n",
        "            print(\"✅ No wide tables → Using LibreOffice pipeline\")\n",
        "            tmp_dir = tempfile.mkdtemp(prefix=\"doc2pdf_\")\n",
        "            subprocess.check_call([\n",
        "                \"soffice\", \"--headless\", \"--convert-to\", \"pdf\",\n",
        "                \"--outdir\", tmp_dir, docx_path\n",
        "            ])\n",
        "\n",
        "            pdf_files = glob.glob(os.path.join(tmp_dir, \"*.pdf\"))\n",
        "            if not pdf_files:\n",
        "                raise RuntimeError(\"DOCX→PDF failed, no PDF generated.\")\n",
        "            pdf_path = pdf_files[0]\n",
        "\n",
        "        print(f\"✅ PDF ready at: {pdf_path}\", '3')\n",
        "\n",
        "        # ✅ PDF → PNG(s)\n",
        "        pages = convert_from_path(pdf_path, dpi=dpi)\n",
        "        print(f\"pages: {len(pages)}\", \"4\")\n",
        "\n",
        "        if len(pages) == 1:\n",
        "            if not out_path.lower().endswith(\".png\"):\n",
        "                out_path += \".png\"\n",
        "            pages[0].save(out_path, \"PNG\")\n",
        "            print(f\"✅ Converted single page image: {out_path}\")\n",
        "            return out_path\n",
        "\n",
        "        # Multiple pages → ask user\n",
        "        print(\"Document has multiple pages. Choose output format:\")\n",
        "        print(\"1. Single PNG (all pages merged vertically)\")\n",
        "        print(\"2. Separate PNG per page (zipped)\")\n",
        "        choice = input(\"Enter choice (1/2): \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            widths, heights = zip(*(p.size for p in pages))\n",
        "            total_height = sum(heights)\n",
        "            max_width = max(widths)\n",
        "            merged_img = Image.new(\"RGB\", (max_width, total_height), \"white\")\n",
        "            y = 0\n",
        "            for p in pages:\n",
        "                merged_img.paste(p, (0, y))\n",
        "                y += p.height\n",
        "            if not out_path.lower().endswith(\".png\"):\n",
        "                out_path += \".png\"\n",
        "            merged_img.save(out_path, \"PNG\")\n",
        "            print(f\"✅ Merged multi-page image saved: {out_path}\")\n",
        "            return out_path\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            if not out_path.lower().endswith(\".zip\"):\n",
        "                out_path += \".zip\"\n",
        "            img_dir = tempfile.mkdtemp(prefix=\"doc2img_\")\n",
        "            image_files = []\n",
        "            for i, page in enumerate(pages, 1):\n",
        "                img_path = os.path.join(img_dir, f\"page_{i}.png\")\n",
        "                page.save(img_path, \"PNG\")\n",
        "                image_files.append(img_path)\n",
        "\n",
        "            with zipfile.ZipFile(out_path, \"w\") as zipf:\n",
        "                for f in image_files:\n",
        "                    zipf.write(f, os.path.basename(f))\n",
        "            print(f\"✅ Separate page images saved as zip: {out_path}\")\n",
        "            return out_path\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid choice\")\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error converting DOCX to PNG: {e}\") from e\n",
        "    finally:\n",
        "        if tmp_dir and os.path.exists(tmp_dir):\n",
        "            shutil.rmtree(tmp_dir, ignore_errors=True)\n",
        "\n",
        "def doc_to_csv(input_path, csv_path):\n",
        "    try:\n",
        "        # Step 1: normalize DOC → DOCX\n",
        "        docx_path = doc_to_docx_image(input_path)\n",
        "        doc = Document(docx_path)\n",
        "\n",
        "        # 🚫 Step 2: Reject if any images exist\n",
        "        for rel in doc.part.rels.values():\n",
        "            if \"image\" in rel.target_ref:\n",
        "                raise RuntimeError(\"❌ Not supported: Document contains images, cannot export to CSV.\")\n",
        "\n",
        "        # Step 3: Scan all tables first\n",
        "        valid_tables = []\n",
        "        for table in doc.tables:\n",
        "            col_count = len(table.rows[0].cells) if table.rows else 0\n",
        "            row_count = len(table.rows)\n",
        "\n",
        "            if col_count < 4 or row_count < 2:\n",
        "                continue\n",
        "\n",
        "            meaningful_rows = 0\n",
        "            for row in table.rows:\n",
        "                texts = [c.text.strip() for c in row.cells if c.text.strip()]\n",
        "                if len(texts) >= (col_count // 2) and any(len(t) >= 3 for t in texts):\n",
        "                    meaningful_rows += 1\n",
        "\n",
        "            if meaningful_rows >= 2:\n",
        "                valid_tables.append(table)\n",
        "\n",
        "        if not valid_tables:\n",
        "            raise RuntimeError(\"❌ Not supported: No valid table (≥4 columns, ≥2 rows, with real data).\")\n",
        "\n",
        "        # ✅ Step 4: Now open CSV only if conversion is confirmed\n",
        "        with open(csv_path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as out:\n",
        "            writer = csv.writer(out)\n",
        "            for table in valid_tables:\n",
        "                for row in table.rows:\n",
        "                    row_data = []\n",
        "                    for c in row.cells:\n",
        "                        text = c.text.replace(\"\\n\", \" \").strip()\n",
        "                        if text:\n",
        "                            text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
        "                        row_data.append(text)\n",
        "                    writer.writerow(row_data)\n",
        "\n",
        "        return csv_path\n",
        "\n",
        "    except Exception as e:\n",
        "        # ❌ Ensure partial/empty CSV is deleted if error\n",
        "        if os.path.exists(csv_path):\n",
        "            os.remove(csv_path)\n",
        "        raise RuntimeError(f\"Error converting DOC/DOCX to CSV: {e}\") from e\n",
        "\n",
        "def doc_to_xls(input_path, xls_path):\n",
        "    try:\n",
        "        # ✅ DOC → DOCX ensure\n",
        "        docx_path = doc_to_docx_image(input_path)\n",
        "\n",
        "        # ✅ Load DOCX\n",
        "        doc = Document(docx_path)\n",
        "\n",
        "        # ✅ Image check (unsupported for XLS export)\n",
        "        for shape in doc.inline_shapes:\n",
        "            raise RuntimeError(\"❌ Not supported: Document contains images, cannot convert to XLS.\")\n",
        "\n",
        "        wrote_any = False\n",
        "        valid_table_found = False\n",
        "\n",
        "        wb = Workbook(write_only=True)\n",
        "        ws = wb.create_sheet(\"Sheet1\")\n",
        "\n",
        "        # ✅ Table scan\n",
        "        for table in doc.tables:\n",
        "            if not table.rows or len(table.rows[0].cells) < 4:\n",
        "                continue  # skip invalid\n",
        "            if len(table.rows) < 2:\n",
        "                continue  # need at least 2 rows\n",
        "\n",
        "            # check real data\n",
        "            has_data = any(any(c.text.strip() for c in row.cells) for row in table.rows)\n",
        "            if not has_data:\n",
        "                continue\n",
        "\n",
        "            valid_table_found = True\n",
        "            for row in table.rows:\n",
        "                row_data = []\n",
        "                for c in row.cells:\n",
        "                    text = c.text.replace(\"\\n\", \" \").strip()\n",
        "                    if text:\n",
        "                        text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
        "                    row_data.append(text)\n",
        "                ws.append(row_data)\n",
        "                wrote_any = True\n",
        "\n",
        "        # ❌ No valid table\n",
        "        if not valid_table_found:\n",
        "            raise RuntimeError(\"❌ Not supported: No valid table (≥4 columns, ≥2 rows, with real data).\")\n",
        "\n",
        "        if not wrote_any:\n",
        "            raise RuntimeError(\"❌ Table found but empty, cannot convert.\")\n",
        "\n",
        "        wb.save(xls_path)\n",
        "        return xls_path\n",
        "\n",
        "    except Exception as e:\n",
        "        # ⚠️ Agar error ho to partial file delete\n",
        "        if os.path.exists(xls_path):\n",
        "            os.remove(xls_path)\n",
        "        raise RuntimeError(f\"Error converting DOC/DOCX to XLSX: {e}\") from e\n",
        "\n",
        "def doc_to_json(input_path, json_path):\n",
        "    try:\n",
        "        # ✅ DOC → DOCX ensure\n",
        "        docx_path = doc_to_docx_image(input_path)\n",
        "\n",
        "        # ✅ Load DOCX\n",
        "        doc = Document(docx_path)\n",
        "\n",
        "        # ✅ Step 3: Image check\n",
        "        for rel in doc.part.rels.values():\n",
        "            if \"image\" in rel.reltype:\n",
        "                raise RuntimeError(\"❌ Not supported: Document contains images.\")\n",
        "\n",
        "        # ✅ Step 4: Raw text join\n",
        "        raw_text = \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "        # ✅ Step 5: Detect JSON-like content\n",
        "        if raw_text.strip().startswith((\"{\", \"[\")):\n",
        "            try:\n",
        "                parsed = json.loads(raw_text)\n",
        "                with open(json_path, \"w\", encoding=\"utf-8\") as out:\n",
        "                    json.dump(parsed, out, ensure_ascii=False, indent=2)\n",
        "                return json_path\n",
        "            except Exception:\n",
        "                pass  # not valid JSON, fallback\n",
        "\n",
        "        # ✅ Step 6: Detect table data\n",
        "        valid_tables = []\n",
        "        for table in doc.tables:\n",
        "            if len(table.columns) >= 4 and len(table.rows) >= 2:\n",
        "                table_data = []\n",
        "                for row in table.rows:\n",
        "                    row_data = []\n",
        "                    for c in row.cells:\n",
        "                        text = c.text.replace(\"\\n\", \" \").strip()\n",
        "                        if text:\n",
        "                            text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
        "                        row_data.append(text)\n",
        "                    table_data.append(row_data)\n",
        "                valid_tables.append(table_data)\n",
        "\n",
        "        # ✅ Step 7: Detect tab/comma separated plain text (like csv.docx)\n",
        "        if not valid_tables:\n",
        "            lines = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
        "            table_data = []\n",
        "            for line in lines:\n",
        "                if \",\" in line or \"\\t\" in line or \";\" in line:\n",
        "                    parts = re.split(r'[\\t,;]', line)\n",
        "                    if len(parts) >= 4:\n",
        "                        table_data.append([p.strip() for p in parts])\n",
        "            if table_data:\n",
        "                valid_tables.append(table_data)\n",
        "\n",
        "        # ✅ Step 8: Final check\n",
        "        if not valid_tables:\n",
        "            raise RuntimeError(\"❌ Not supported: No valid JSON or tabular data found.\")\n",
        "\n",
        "        payload = {\"tables\": valid_tables}\n",
        "        with open(json_path, \"w\", encoding=\"utf-8\") as out:\n",
        "            json.dump(payload, out, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return json_path\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # ⚠️ Error aane par partial file delete\n",
        "        if os.path.exists(json_path):\n",
        "            os.remove(json_path)\n",
        "        raise RuntimeError(f\"Error converting DOC/DOCX to JSON: {e}\") from e\n",
        "\n",
        "# ---------------- CLI MENU ---------------- #\n",
        "\n",
        "def main():\n",
        "    # For Colab file handling\n",
        "    try:\n",
        "        from google.colab import files\n",
        "    except ImportError:\n",
        "        files = None\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n==== DOCX File Converter =====\")\n",
        "        print(\"1. Convert DOCX File\")\n",
        "        print(\"2. Exit\")\n",
        "        choice = input(\"Enter choice: \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            # Upload file (Colab) or enter path (offline)\n",
        "            if files:\n",
        "                uploaded = files.upload()\n",
        "                pdf_file = list(uploaded.keys())[0]\n",
        "                #pdf_file = input(\"Enter path of DOCX/DOC file: \").strip()\n",
        "            else:\n",
        "                pdf_file = input(\"Enter path of DOC/DOCX file: \").strip()\n",
        "\n",
        "            print(\"\\nSupported conversions: \")\n",
        "            print(\"1. DOC → PDF\")\n",
        "            print(\"2. DOC → TXT\")\n",
        "            print(\"3. DOC → PNG (image)\")\n",
        "            print(\"4. DOC → CSV\")\n",
        "            print(\"5. DOC → XLSX\")\n",
        "            print(\"6. DOC → JSON\")\n",
        "            fmt_choice = input(\"Select target format (1-6): \").strip()\n",
        "\n",
        "            base, _ = os.path.splitext(pdf_file)\n",
        "            out_file = None\n",
        "\n",
        "            try:\n",
        "                if fmt_choice == \"1\":\n",
        "                    out_file = base + \".pdf\"\n",
        "                    doc_to_pdf(pdf_file, out_file)\n",
        "                elif fmt_choice == \"2\":\n",
        "                    out_file = base + \".txt\"\n",
        "                    doc_to_txt(pdf_file, out_file)\n",
        "                elif fmt_choice == \"3\":\n",
        "                    out_file = base + \".png\"\n",
        "                    out_dir = base + \"_images\"\n",
        "                    # Modified to use the corrected doc_to_image\n",
        "                    out_file = doc_to_image(pdf_file, out_file)\n",
        "                elif fmt_choice == \"4\":\n",
        "                    out_file = base + \".csv\"\n",
        "                    doc_to_csv(pdf_file, out_file)\n",
        "                elif fmt_choice == \"5\":\n",
        "                    out_file = base + \".xlsx\"\n",
        "                    doc_to_xls(pdf_file, out_file)\n",
        "                elif fmt_choice == \"6\":\n",
        "                    out_file = base + \".json\"\n",
        "                    doc_to_json(pdf_file, out_file)\n",
        "                else:\n",
        "                    print(\"❌ Invalid choice!\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"✅ Converted successfully: {out_file}\")\n",
        "\n",
        "                # Schedule auto-delete after 5 minutes\n",
        "                if out_file:\n",
        "                    schedule_delete(out_file)\n",
        "\n",
        "                # Colab download option\n",
        "                if files and out_file:\n",
        "                    if os.path.isdir(out_file): # if output is a directory (images)\n",
        "                         print(f\"💡 Multiple files saved in {out_file}. You may need to zip and download manually.\")\n",
        "                         # Example of zipping and offering download (requires zip installed)\n",
        "                         # !zip -r {out_file}.zip {out_file}\n",
        "                         # files.download(f\"{out_file}.zip\")\n",
        "                    else:\n",
        "                         files.download(out_file)\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"❌ Conversion failed:\", e)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            print(\"👋 Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"❌ Invalid choice!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3KxZXLBklak"
      },
      "source": [
        "# Install EasyOCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PXKLkGipkxgu"
      },
      "outputs": [],
      "source": [
        "!pip install easyocr\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!sudo apt-get install tesseract-ocr-hin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpgeFel2tW9d"
      },
      "source": [
        "# Json Convertor <small>json_converter.py</small>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48hicsZktnma"
      },
      "outputs": [],
      "source": [
        "# json_converters.py\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib import colors\n",
        "from pdf2image import convert_from_path\n",
        "from tqdm import tqdm   # pip install tqdm\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from docx import Document\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "import threading\n",
        "import json, csv\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "import json\n",
        "from reportlab.pdfgen import canvas\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "import os, zipfile, textwrap, uuid\n",
        "import tempfile\n",
        "import shutil\n",
        "import ijson\n",
        "import pdfkit\n",
        "import pypandoc\n",
        "from weasyprint import HTML\n",
        "\n",
        "def _ensure_dir(file_path):\n",
        "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "def register_unicode_font(font_files=None):\n",
        "    \"\"\"\n",
        "    Registers first available TrueType font for Unicode support.\n",
        "    Returns the font name to be used in PDF.\n",
        "    \"\"\"\n",
        "    if font_files is None:\n",
        "        font_files = [\"NotoSans-Regular.ttf\", \"NotoSansDevanagari.ttf\"]\n",
        "    for f in font_files:\n",
        "        if os.path.exists(f):\n",
        "            try:\n",
        "                pdfmetrics.registerFont(TTFont(\"UnicodeFont\", f))\n",
        "                return \"UnicodeFont\"\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not register font {f}: {e}\")\n",
        "    print(\"⚠️ No Unicode font found, using default Helvetica\")\n",
        "    return \"Helvetica\"\n",
        "\n",
        "#------------------------- Json To PDF -----------------------------------------------#\n",
        "\n",
        "def json_to_pdf(json_path, output_pdf):\n",
        "    try:\n",
        "\n",
        "        # --- Only allow .json files ---\n",
        "        if not json_path.lower().endswith(\".json\"):\n",
        "            print(f\"❌ Unsupported file type: {json_path}. Only .json files are supported.\")\n",
        "\n",
        "        if not os.path.exists(json_path):\n",
        "            raise RuntimeError(\"Input file not found: \" + json_path)\n",
        "\n",
        "        # Detect JSON type: normal vs JSON lines\n",
        "        data_list = []\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            first_char = f.read(1)\n",
        "            f.seek(0)\n",
        "            if first_char == \"{\":\n",
        "                # Could be JSON Lines or single JSON object\n",
        "                try:\n",
        "                    # Try loading whole file as a JSON object\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, dict):\n",
        "                        data_list.append(data)\n",
        "                    elif isinstance(data, list):\n",
        "                        data_list = data\n",
        "                except json.JSONDecodeError:\n",
        "                    # Treat as JSON Lines\n",
        "                    f.seek(0)\n",
        "                    for line in f:\n",
        "                        line = line.strip()\n",
        "                        if line:\n",
        "                            data_list.append(json.loads(line))\n",
        "            elif first_char == \"[\":\n",
        "                data_list = json.load(f)\n",
        "            else:\n",
        "                raise RuntimeError(\"Invalid JSON format\")\n",
        "\n",
        "        # Convert to pretty JSON string\n",
        "        json_text = json.dumps(data_list, indent=4, ensure_ascii=False)\n",
        "\n",
        "        # HTML content for PDF\n",
        "        html_content = f\"\"\"\n",
        "        <html>\n",
        "        <head>\n",
        "            <meta charset=\"utf-8\">\n",
        "            <style>\n",
        "                body {{ font-family: 'Noto Sans', 'Devanagari Sans', monospace; font-size:12pt; }}\n",
        "                pre {{ white-space: pre-wrap; word-wrap: break-word; }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <pre>{json_text}</pre>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        tmp_html = tempfile.mktemp(suffix=\".html\")\n",
        "        with open(tmp_html, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html_content)\n",
        "\n",
        "        pdfkit.from_file(tmp_html, output_pdf)\n",
        "        os.remove(tmp_html)\n",
        "        return output_pdf\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error{e}\")\n",
        "\n",
        "#------------------------- Json To DOCX ----------------------------------------#\n",
        "def json_to_doc(json_file, output_file):\n",
        "    try:\n",
        "        if not json_file.endswith(\".json\"):\n",
        "            raise RuntimeError(\"❌ Only JSON files are supported!\")\n",
        "\n",
        "        if not os.path.exists(json_file):\n",
        "            raise RuntimeError(\"❌ File not found!\")\n",
        "\n",
        "        data_list = []\n",
        "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            first_char = f.read(1)\n",
        "            f.seek(0)\n",
        "            if first_char == \"{\":\n",
        "                try:\n",
        "                    # Normal JSON\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, dict):\n",
        "                        data_list.append(data)\n",
        "                    elif isinstance(data, list):\n",
        "                        data_list = data\n",
        "                except json.JSONDecodeError:\n",
        "                    # NDJSON\n",
        "                    f.seek(0)\n",
        "                    for line in f:\n",
        "                        line = line.strip()\n",
        "                        if line:\n",
        "                            data_list.append(json.loads(line))\n",
        "            elif first_char == \"[\":\n",
        "                data_list = json.load(f)\n",
        "            else:\n",
        "                raise RuntimeError(\"❌ Invalid JSON format\")\n",
        "\n",
        "        if not data_list:\n",
        "            raise RuntimeError(\"❌ No data found in JSON.\")\n",
        "\n",
        "        # --- Create DOCX ---\n",
        "        doc = Document()\n",
        "        style = doc.styles['Normal']\n",
        "        font = style.font\n",
        "        font.name = 'Nirmala UI'   # Hindi-friendly font (fallback)\n",
        "        font.size = Pt(11)\n",
        "        style._element.rPr.rFonts.set(qn('w:eastAsia'), 'Nirmala UI')\n",
        "\n",
        "        doc.add_heading(\"JSON Data Export\", level=1)\n",
        "\n",
        "        # --- Output formatting ---\n",
        "        if isinstance(data_list, list) and all(isinstance(item, dict) for item in data_list):\n",
        "            # Multiple dicts → print each as formatted block\n",
        "            for idx, obj in enumerate(data_list, 1):\n",
        "                doc.add_paragraph(f\"Record {idx}:\", style='Heading 2')\n",
        "                pretty = json.dumps(obj, indent=4, ensure_ascii=False)\n",
        "                doc.add_paragraph(pretty)\n",
        "        else:\n",
        "            # Single object or nested → dump as pretty JSON\n",
        "            pretty = json.dumps(data_list, indent=4, ensure_ascii=False)\n",
        "            doc.add_paragraph(pretty)\n",
        "\n",
        "        # --- Save DOCX ---\n",
        "        doc.save(output_file)\n",
        "        return output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ Conversion failed: {e}\")\n",
        "\n",
        "\n",
        "# ------------------ Json TO Image ------------------------------------------\n",
        "def is_ndjson(json_path):\n",
        "    \"\"\"Detect NDJSON (multiple lines with valid JSON objects)\"\"\"\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        try:\n",
        "            json.load(f)   # normal JSON\n",
        "            return False\n",
        "        except json.JSONDecodeError:\n",
        "            return True\n",
        "\n",
        "def json_to_image(json_path, output_path):\n",
        "    try:\n",
        "        if not json_path.endswith(\".json\"):\n",
        "            raise RuntimeError(\"❌ Only JSON files are supported!\")\n",
        "        if not os.path.exists(json_path):\n",
        "            raise RuntimeError(\"❌ File not found!\")\n",
        "\n",
        "        ndjson_mode = is_ndjson(json_path)\n",
        "\n",
        "        print(\"Choose conversion mode:\")\n",
        "        print(\"1. Single big image (whole JSON in one PNG)\")\n",
        "        print(\"2. Split into 30-line chunks (ZIP of PNGs)\")\n",
        "        choice = input(\"Enter choice (1/2): \").strip()\n",
        "\n",
        "        if ndjson_mode:\n",
        "            print(\"📂 NDJSON detected → routing via json_to_pdf()...\")\n",
        "            tmp_pdf = tempfile.mktemp(suffix=\".pdf\")\n",
        "            json_to_pdf(json_path, tmp_pdf)   # use your fast code\n",
        "\n",
        "            # convert PDF → images\n",
        "            images = convert_from_path(tmp_pdf, dpi=100)\n",
        "\n",
        "            if choice == \"1\":\n",
        "                # merge pages into one long PNG\n",
        "                total_height = sum(i.height for i in images)\n",
        "                max_width = max(i.width for i in images)\n",
        "                big_img = Image.new(\"RGB\", (max_width, total_height), \"white\")\n",
        "                y_offset = 0\n",
        "                for img in images:\n",
        "                    big_img.paste(img, (0, y_offset))\n",
        "                    y_offset += img.height\n",
        "                big_img.save(output_path, \"PNG\")\n",
        "                print(f\"✅ NDJSON → single long image: {output_path}\")\n",
        "                return output_path\n",
        "            else:\n",
        "                # each page separate → zip\n",
        "                tmpdir = tempfile.mkdtemp()\n",
        "                img_files = []\n",
        "                base, ext = os.path.splitext(output_path)\n",
        "                for i, page in enumerate(images, 1):\n",
        "                    out_file = os.path.join(tmpdir, f\"page_{i}.png\")\n",
        "                    page.save(out_file, \"PNG\")\n",
        "                    img_files.append(out_file)\n",
        "\n",
        "                zip_path = output_path.replace(\".png\", \".zip\")\n",
        "                with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
        "                    for img in img_files:\n",
        "                        zipf.write(img, os.path.basename(img))\n",
        "\n",
        "                print(f\"✅ NDJSON → images zipped: {zip_path}\")\n",
        "                return zip_path\n",
        "\n",
        "        else:\n",
        "            # ✅ Normal JSON → existing HTML flow\n",
        "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data_json = json.load(f)\n",
        "            if isinstance(data_json, dict):\n",
        "                data_json = [data_json]\n",
        "            pretty_json = json.dumps(data_json, indent=4, ensure_ascii=False)\n",
        "\n",
        "            if choice == \"1\":\n",
        "                return save_html_as_image(pretty_json, output_path, long_mode=True)\n",
        "            else:\n",
        "                lines = pretty_json.split(\"\\n\")\n",
        "                tmpdir = tempfile.mkdtemp()\n",
        "                img_files = []\n",
        "                chunk_size = 30\n",
        "                for i in range(0, len(lines), chunk_size):\n",
        "                    chunk = \"\\n\".join(lines[i:i + chunk_size])\n",
        "                    img_file = os.path.join(tmpdir, f\"chunk_{i//chunk_size + 1}.png\")\n",
        "                    save_html_as_image(chunk, img_file, long_mode=False)\n",
        "                    img_files.append(img_file)\n",
        "\n",
        "                zip_path = output_path.replace(\".png\", \".zip\")\n",
        "                with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
        "                    for img in img_files:\n",
        "                        zipf.write(img, os.path.basename(img))\n",
        "\n",
        "                print(f\"✅ Converted successfully (chunks zipped): {zip_path}\")\n",
        "                return zip_path\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ Conversion failed: {e}\")\n",
        "\n",
        "def save_html_as_image(text, out_path, long_mode=False):\n",
        "    html_content = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <style>\n",
        "            body {{\n",
        "                font-family: 'Nirmala UI','Noto Sans Devanagari','Noto Sans','Arial Unicode MS',sans-serif;\n",
        "                font-size: 14pt;\n",
        "                white-space: pre-wrap;\n",
        "            }}\n",
        "            pre {{\n",
        "                white-space: pre-wrap;\n",
        "                word-wrap: break-word;\n",
        "            }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body><pre>{text}</pre></body></html>\n",
        "    \"\"\"\n",
        "    tmp_html = tempfile.mktemp(suffix=\".html\")\n",
        "    with open(tmp_html, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    tmp_pdf = out_path.replace(\".png\", f\"_{uuid.uuid4().hex}.pdf\")\n",
        "    HTML(tmp_html).write_pdf(tmp_pdf)\n",
        "    images = convert_from_path(tmp_pdf, dpi=100)\n",
        "\n",
        "    if long_mode and len(images) > 1:\n",
        "        total_height = sum(i.height for i in images)\n",
        "        max_width = max(i.width for i in images)\n",
        "        big_img = Image.new(\"RGB\", (max_width, total_height), \"white\")\n",
        "        y_offset = 0\n",
        "        for img in images:\n",
        "            big_img.paste(img, (0, y_offset))\n",
        "            y_offset += img.height\n",
        "        big_img.save(out_path, \"PNG\")\n",
        "        print(f\"✅ Saved single long image: {out_path}\")\n",
        "        return out_path\n",
        "    else:\n",
        "        images[0].save(out_path, \"PNG\")\n",
        "        print(f\"✅ Saved image: {out_path}\")\n",
        "        return out_path\n",
        "\n",
        "#----------------------- Json To TXT-------------------------------#\n",
        "def json_to_txt(json_path, txt_path):\n",
        "    \"\"\"\n",
        "    Convert JSON/NDJSON to TXT.\n",
        "    - Handles large files (50MB+).\n",
        "    - Simple JSON => pretty indented text.\n",
        "    - NDJSON => record-by-record streaming.\n",
        "    - UTF-8 safe (Hindi + English).\n",
        "    \"\"\"\n",
        "    try :\n",
        "\n",
        "        if not json_path.endswith(\".json\"):\n",
        "              raise RuntimeError(\"❌ Only JSON files are supported!\")\n",
        "\n",
        "        if not os.path.exists(json_path):\n",
        "              raise RuntimeError(\"❌ File not found!\")\n",
        "\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f, \\\n",
        "            open(txt_path, \"w\", encoding=\"utf-8\") as out:\n",
        "\n",
        "            try:\n",
        "                # Try loading as a whole JSON (simple JSON)\n",
        "                data = json.load(f)\n",
        "                # pretty print\n",
        "                pretty = json.dumps(data, ensure_ascii=False, indent=4)\n",
        "                out.write(pretty + \"\\n\")\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                # Fallback: NDJSON mode\n",
        "                f.seek(0)\n",
        "                line_no = 0\n",
        "                for line in f:\n",
        "                    line_no += 1\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "                    try:\n",
        "                        obj = json.loads(line)\n",
        "                        # pretty-print each object with indentation\n",
        "                        pretty = json.dumps(obj, ensure_ascii=False, indent=4)\n",
        "                        out.write(f\"--- Record {line_no} ---\\n{pretty}\\n\\n\")\n",
        "                    except Exception:\n",
        "                        out.write(f\"⚠️ Skipping bad line {line_no}: {line[:50]}\\n\")\n",
        "\n",
        "        return txt_path\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ Conversion failed: {e}\")\n",
        "\n",
        "#-------------------- Json To CSV ----------------------------------#\n",
        "def json_to_csv(json_path, csv_path):\n",
        "    \"\"\"\n",
        "    Convert JSON or NDJSON into CSV.\n",
        "    - Handles large files (50MB+).\n",
        "    - UTF-8 safe (Hindi + English).\n",
        "    - Supports list-of-objects JSON & NDJSON.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not json_path.endswith(\".json\"):\n",
        "                  raise RuntimeError(\"❌ Only JSON files are supported!\")\n",
        "\n",
        "        if not os.path.exists(json_path):\n",
        "                  raise RuntimeError(\"❌ File not found!\")\n",
        "\n",
        "        with open(csv_path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as out:\n",
        "            # utf-8-sig ensures Excel also reads Hindi properly\n",
        "            writer = None\n",
        "\n",
        "            try:\n",
        "                # ---- Try as normal JSON (array of objects) ----\n",
        "                with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                if isinstance(data, dict):\n",
        "                    # if root is dict → wrap into list\n",
        "                    data = [data]\n",
        "\n",
        "                # Ensure it's list of dicts\n",
        "                if not isinstance(data, list):\n",
        "                    raise ValueError(\"JSON is not a list-of-objects\")\n",
        "\n",
        "                # Write CSV\n",
        "                for idx, obj in enumerate(data):\n",
        "                    if writer is None:\n",
        "                        writer = csv.DictWriter(out, fieldnames=list(obj.keys()))\n",
        "                        writer.writeheader()\n",
        "                    writer.writerow(obj)\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                # ---- NDJSON fallback (streaming) ----\n",
        "                with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    for obj in ijson.items(f, \"\", multiple_values=True):\n",
        "                        if isinstance(obj, dict):\n",
        "                            if writer is None:\n",
        "                                writer = csv.DictWriter(out, fieldnames=list(obj.keys()))\n",
        "                                writer.writeheader()\n",
        "                            writer.writerow(obj)\n",
        "                        else:\n",
        "                            # if NDJSON line is not dict\n",
        "                            if writer is None:\n",
        "                                writer = csv.writer(out)\n",
        "                                writer.writerow([\"value\"])\n",
        "                            writer.writerow([obj])\n",
        "\n",
        "        return csv_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Conversion failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ----------------------- JSON To XLS --------------------------------------#\n",
        "def flatten_json(obj, parent_key=\"\", sep=\".\"):\n",
        "    \"\"\"Flatten nested JSON (dicts/lists) into key-value pairs.\"\"\"\n",
        "    items = []\n",
        "    if isinstance(obj, dict):\n",
        "        for k, v in obj.items():\n",
        "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "            items.extend(flatten_json(v, new_key, sep=sep).items())\n",
        "    elif isinstance(obj, list):\n",
        "        for i, v in enumerate(obj):\n",
        "            new_key = f\"{parent_key}[{i}]\"\n",
        "            items.extend(flatten_json(v, new_key, sep=sep).items())\n",
        "    else:\n",
        "        items.append((parent_key, obj))\n",
        "    return dict(items)\n",
        "\n",
        "def json_to_xls(json_path, xls_path):\n",
        "    \"\"\"\n",
        "    Convert JSON or NDJSON into XLSX.\n",
        "    - Handles large files via streaming\n",
        "    - UTF-8 safe (Hindi/English)\n",
        "    - Flattens nested JSON\n",
        "    \"\"\"\n",
        "    try:\n",
        "        wb = Workbook(write_only=True)\n",
        "        ws = wb.create_sheet(\"Sheet1\")\n",
        "\n",
        "        headers_written = False\n",
        "        headers = None\n",
        "\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            try:\n",
        "                # ---- simple JSON ----\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, dict):\n",
        "                    data = [data]\n",
        "                if not isinstance(data, list):\n",
        "                    raise ValueError(\"JSON is not a list of objects.\")\n",
        "\n",
        "                for obj in data:\n",
        "                    if not isinstance(obj, dict):\n",
        "                        continue\n",
        "                    flat = flatten_json(obj)\n",
        "                    if not headers_written:\n",
        "                        headers = list(flat.keys())\n",
        "                        ws.append(headers)\n",
        "                        headers_written = True\n",
        "                    ws.append([flat.get(h, \"\") for h in headers])\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                # ---- NDJSON ----\n",
        "                f.seek(0)\n",
        "                for obj in ijson.items(f, \"\", multiple_values=True):\n",
        "                    if not isinstance(obj, dict):\n",
        "                        continue\n",
        "                    flat = flatten_json(obj)\n",
        "                    if not headers_written:\n",
        "                        headers = list(flat.keys())\n",
        "                        ws.append(headers)\n",
        "                        headers_written = True\n",
        "                    ws.append([flat.get(h, \"\") for h in headers])\n",
        "\n",
        "        wb.save(xls_path)\n",
        "        wb.close()\n",
        "\n",
        "        if not os.path.exists(xls_path):\n",
        "            return None\n",
        "        return xls_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ Error:\", e)\n",
        "        return None\n",
        "\n",
        "def schedule_delete(file_path, delay=300):  # 5 minutes\n",
        "    def delete_file():\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                if os.path.isdir(file_path):\n",
        "                    import shutil\n",
        "                    shutil.rmtree(file_path)\n",
        "                else:\n",
        "                    os.remove(file_path)\n",
        "                print(f\"🗑 Deleted: {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to delete {file_path}: {e}\")\n",
        "    threading.Timer(delay, delete_file).start()\n",
        "\n",
        "# ---------------- CLI MENU ---------------- #\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        from google.colab import files\n",
        "    except ImportError:\n",
        "        files = None\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n==== File Converter =====\")\n",
        "        print(\"1. Convert JSON File\")\n",
        "        print(\"2. Exit\")\n",
        "        choice = input(\"Enter choice: \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "\n",
        "            # Upload file (Colab) or enter path (offline)\n",
        "            if files:\n",
        "                #uploaded = files.upload()\n",
        "                #pdf_file = list(uploaded.keys())[0]\n",
        "                json_file = input(\"Enter path of JSON file: \").strip()\n",
        "            else:\n",
        "                json_file = input(\"Enter path of DOC/DOCX file: \").strip()\n",
        "\n",
        "            print(\"\\nSupported conversions: \")\n",
        "            print(\"1. JSON → PDF\")\n",
        "            print(\"2. JSON → DOCX\")\n",
        "            print(\"3. JSON → PNG (image)\")\n",
        "            print(\"4. JSON → TXT\")\n",
        "            print(\"5. JSON → CSV\")\n",
        "            print(\"6. JSON → XLSX\")\n",
        "            fmt_choice = input(\"Select target format (1-6): \").strip()\n",
        "\n",
        "            base, _ = os.path.splitext(json_file)\n",
        "            out_file = None\n",
        "\n",
        "            try:\n",
        "                if fmt_choice == \"1\":\n",
        "                    out_file = base + \".pdf\"\n",
        "                    json_to_pdf(json_file, out_file)\n",
        "                elif fmt_choice == \"2\":\n",
        "                    out_file = base + \".docx\"\n",
        "                    json_to_doc(json_file, out_file)\n",
        "                elif fmt_choice == \"3\":\n",
        "                    out_file = base + \".png\"\n",
        "                    json_to_image(json_file, out_file)\n",
        "                elif fmt_choice == \"4\":\n",
        "                    out_file = base + \".txt\"\n",
        "                    json_to_txt(json_file, out_file)\n",
        "                elif fmt_choice == \"5\":\n",
        "                    out_file = base + \".csv\"\n",
        "                    json_to_csv(json_file, out_file)\n",
        "                elif fmt_choice == \"6\":\n",
        "                    out_file = base + \".xlsx\"\n",
        "                    json_to_xls(json_file, out_file)\n",
        "                else:\n",
        "                    print(\"❌ Invalid choice!\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"✅ Converted successfully: {out_file}\")\n",
        "\n",
        "                if out_file:\n",
        "                    schedule_delete(out_file)\n",
        "\n",
        "                if files and out_file:\n",
        "                    files.download(out_file)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"❌ Conversion failed:\", e)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            print(\"👋 Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"❌ Invalid choice!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCbrzG99GHdE"
      },
      "source": [
        "# Image Converter Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF3jJM07GimI"
      },
      "outputs": [],
      "source": [
        "# image_converters.py\n",
        "import os\n",
        "from typing import List, Union\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import pytesseract\n",
        "from langdetect import detect\n",
        "import langid\n",
        "import easyocr # Import easyocr\n",
        "\n",
        "# Initialize EasyOCR reader (load models once)\n",
        "# Specify languages you expect to encounter, e.g., ['en', 'hi', 'es', 'fr', 'de']\n",
        "# Add more languages as needed. 'en' is default.\n",
        "# Download language models if they are not present.\n",
        "_EASYOCR_READER = None\n",
        "_EASYOCR_LANGS = []\n",
        "\n",
        "def _get_easyocr(langs=['en']):\n",
        "    global _EASYOCR_READER, _EASYOCR_LANGS\n",
        "    # Re-initialize if reader is not set or if requested languages are different from currently loaded languages\n",
        "    if _EASYOCR_READER is None or set(langs) != set(_EASYOCR_LANGS):\n",
        "        try:\n",
        "            import easyocr\n",
        "            # The Reader constructor takes a list of languages directly\n",
        "            _EASYOCR_READER = easyocr.Reader(langs, gpu=False) # Set gpu=True if you have a GPU\n",
        "            _EASYOCR_LANGS = langs # Update the stored languages\n",
        "            print(f\"✅ Initialized EasyOCR with languages: {langs}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing EasyOCR with languages {langs}: {e}\")\n",
        "            _EASYOCR_READER = None\n",
        "            _EASYOCR_LANGS = [] # Clear languages on failure\n",
        "    return _EASYOCR_READER\n",
        "\n",
        "# Preprocessing function (optional but can help)\n",
        "def preprocess_image(image_path, strong=False):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    if strong:\n",
        "        img = img.filter(ImageFilter.MedianFilter(3))\n",
        "        enhancer = ImageEnhance.Contrast(img)\n",
        "        img = enhancer.enhance(2)\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(1.5)\n",
        "    return img\n",
        "\n",
        "def detect_language_from_image(img):\n",
        "    \"\"\"\n",
        "    Quick heuristic: run a tiny OCR (English + few langs),\n",
        "    then detect script with langdetect/langid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use a basic config for quick detection\n",
        "        tmp_text = pytesseract.image_to_string(img, lang=\"eng+hin\", config=\"--psm 6 --oem 3\")\n",
        "        if not tmp_text.strip():\n",
        "            return \"en\" # Default to English if no text detected\n",
        "\n",
        "        # Use langid for better language detection on short text\n",
        "        code, _ = langid.classify(tmp_text)\n",
        "        return code\n",
        "    except Exception as e:\n",
        "        print(f\"Language detection failed: {e}\")\n",
        "        return \"en\" # Fallback to English on error\n",
        "\n",
        "def _langs_for_tesseract(easyocr_langs):\n",
        "    \"\"\"Maps EasyOCR language codes to Tesseract language codes.\"\"\"\n",
        "    tess_map = {\n",
        "        'en': 'eng', 'hi': 'hin', 'es': 'spa', 'fr': 'fra', 'de': 'deu',\n",
        "        'ru': 'rus', 'ja': 'jpn', 'ko': 'kor', 'ch_sim': 'chi_sim', 'ch_tra': 'chi_tra'\n",
        "    }\n",
        "    tess_langs = [tess_map.get(lang, lang) for lang in easyocr_langs]\n",
        "    return \"+\".join(tess_langs)\n",
        "\n",
        "def _parse_langs_for_easyocr(lang_input):\n",
        "    \"\"\"Parses comma-separated or auto language input for EasyOCR.\"\"\"\n",
        "    if lang_input.lower() == 'auto':\n",
        "        # For auto-detection, still need to give EasyOCR *some* languages to load.\n",
        "        # Let's default to English and Hindi for a common use case, but this could be expanded.\n",
        "        return ['en', 'hi']\n",
        "    return [l.strip() for l in lang_input.split(',')]\n",
        "\n",
        "\n",
        "def image_to_txt_ocr(in_path, txt_path, lang=\"auto\"):\n",
        "    \"\"\"\n",
        "    Extracts text from an image using OCR. Supports multiple languages.\n",
        "    Uses EasyOCR and Tesseract, prioritizing EasyOCR for some languages.\n",
        "\n",
        "    Args:\n",
        "        in_path (str): Path to the input image file.\n",
        "        txt_path (str): Path to the output text file.\n",
        "        lang (str): Language code(s) (e.g., 'en', 'hi', 'en,hi'). Use 'auto' for auto-detection.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the output text file if successful, None otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(in_path):\n",
        "            raise FileNotFoundError(f\"❌ File not found: {in_path}\")\n",
        "\n",
        "        # Preprocess image\n",
        "        img = preprocess_image(in_path, strong=False)\n",
        "\n",
        "        # Handle 'auto' language detection\n",
        "        target_langs = []\n",
        "        if lang.lower() == 'auto':\n",
        "            # For auto-detection, load default EasyOCR languages and then try to detect\n",
        "            target_langs_for_easyocr = _parse_langs_for_easyocr(lang)\n",
        "            reader = _get_easyocr(target_langs_for_easyocr) # Load initial languages\n",
        "            if reader:\n",
        "                 # Now try to detect language from the image\n",
        "                 detected = detect_language_from_image(img)\n",
        "                 print(f\"🌐 Detected language: {detected}\")\n",
        "                 # Re-initialize EasyOCR with the detected language + English for robustness\n",
        "                 target_langs = list(set([detected, 'en'])) # Use set to avoid duplicates\n",
        "                 # Ensure the detected language is in the supported list for EasyOCR if not English\n",
        "                 if detected != 'en' and detected not in ['hi', 'es', 'fr', 'de', 'ru', 'ja', 'ko', 'ch_sim', 'ch_tra']: # Add more supported languages if needed\n",
        "                      print(f\"⚠️ Detected language {detected} might not be fully supported by EasyOCR. Using English.\")\n",
        "                      target_langs = ['en'] # Fallback to English\n",
        "                 reader = _get_easyocr(target_langs) # Re-initialize with detected/fallback languages\n",
        "            else:\n",
        "                 # If EasyOCR couldn't even initialize with defaults\n",
        "                 target_langs = ['en'] # Fallback to Tesseract with English\n",
        "        else:\n",
        "            target_langs = _parse_langs_for_easyocr(lang)\n",
        "            reader = _get_easyocr(target_langs) # Initialize with specified languages\n",
        "\n",
        "\n",
        "        text = \"\"\n",
        "        used_engine = \"\"\n",
        "\n",
        "        # --- Attempt with EasyOCR ---\n",
        "        # Ensure reader is not None before using\n",
        "        if reader:\n",
        "            try:\n",
        "                # Use paragraph=True for better formatting\n",
        "                results = reader.readtext(img, detail=0, paragraph=True)\n",
        "                text = \"\\n\".join(results).strip()\n",
        "                used_engine = \"EasyOCR\"\n",
        "                print(f\"✅ OCR extracted text using EasyOCR.\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ EasyOCR failed: {e}\")\n",
        "\n",
        "\n",
        "        # --- Fallback or additional pass with Tesseract ---\n",
        "        if not text or used_engine != \"EasyOCR\": # If EasyOCR failed or was skipped\n",
        "             tess_langs = _langs_for_tesseract(target_langs)\n",
        "             print(f\"Attempting Tesseract with languages: {tess_langs}\")\n",
        "             try:\n",
        "                # Ensure tesseract_cmd is set if not in PATH (common in Colab)\n",
        "                # pytesseract.tesseract_cmd = r'/usr/bin/tesseract' # Uncomment if needed\n",
        "                text = pytesseract.image_to_string(img, lang=tess_langs, config=\"--oem 3 --psm 6\").strip()\n",
        "                used_engine = \"Tesseract\"\n",
        "                print(f\"✅ OCR extracted text using Tesseract.\")\n",
        "             except pytesseract.TesseractNotFoundError:\n",
        "                 print(\"❌ Tesseract is not installed or not in PATH. Please install it.\")\n",
        "                 text = \"\" # Ensure text is empty if Tesseract is not found\n",
        "             except Exception as e:\n",
        "                print(f\"⚠️ Tesseract failed for languages {tess_langs}: {e}\")\n",
        "                text = \"\" # Ensure text is empty if Tesseract also fails\n",
        "\n",
        "\n",
        "        if not text:\n",
        "            raise RuntimeError(\"OCR produced empty text from both engines or both failed.\")\n",
        "\n",
        "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "        print(f\"✅ OCR extracted text saved: {txt_path} (Engine: {used_engine})\")\n",
        "        return txt_path\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "         print(f\"❌ File not found: {e}\")\n",
        "         return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ OCR failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ---------- Image format helpers ----------\n",
        "def image_to_image(in_path, out_path, fmt):\n",
        "    try:\n",
        "        # fmt: \"PNG\", \"JPEG\", etc.\n",
        "        img = Image.open(in_path)\n",
        "        # Convert mode if needed for JPEG\n",
        "        if fmt.upper() in [\"JPEG\", \"JPG\"] and img.mode in (\"RGBA\", \"LA\"):\n",
        "            bg = Image.new(\"RGB\", img.size, (255,255,255))\n",
        "            bg.paste(img, mask=img.split()[-1])\n",
        "            img = bg\n",
        "        img.convert(\"RGB\").save(out_path, fmt.upper())\n",
        "        return out_path\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ Image conversion failed: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# ---- Tiny wrappers (unchanged names) ----\n",
        "def jpg_to_png(in_path, out_path): return image_to_image(in_path, out_path, \"PNG\")\n",
        "def png_to_jpg(in_path, out_path): return image_to_image(in_path, out_path, \"JPEG\")\n",
        "def jpg_to_jpeg(in_path, out_path): return image_to_image(in_path, out_path, \"JPEG\")\n",
        "def png_to_jpeg(in_path, out_path): return image_to_image(in_path, out_path, \"JPEG\")\n",
        "def jpeg_to_png(in_path, out_path): return image_to_image(in_path, out_path, \"PNG\")\n",
        "def jpeg_to_jpg(in_path, out_path): return image_to_image(in_path, out_path, \"JPEG\")\n",
        "def gif_to_png(in_path, out_path): return image_to_image(in_path, out_path, \"PNG\")\n",
        "\n",
        "# OCR wrappers (note: lang now supports \"auto\" / \"en,hi\" / \"eng+hin\")\n",
        "def jpg_to_txt(in_path, out_path, lang=\"auto\"):  return image_to_txt_ocr(in_path, out_path, lang)\n",
        "def jpeg_to_txt(in_path, out_path, lang=\"auto\"): return image_to_txt_ocr(in_path, out_path, lang)\n",
        "def png_to_txt(in_path, out_path, lang=\"auto\"):  return image_to_txt_ocr(in_path, out_path, lang)\n",
        "def gif_to_txt(in_path, out_path, lang=\"auto\"):  return image_to_txt_ocr(in_path, out_path, lang)\n",
        "def tiff_to_txt(in_path, out_path, lang=\"auto\"): return image_to_txt_ocr(in_path, out_path, lang)\n",
        "def bmp_to_txt(in_path, out_path, lang=\"auto\"):  return image_to_txt_ocr(in_path, out_path, lang)\n",
        "\n",
        "\n",
        "# ---------------- CLI MENU ---------------- #\n",
        "def main():\n",
        "    try:\n",
        "        from google.colab import files\n",
        "    except ImportError:\n",
        "        files = None\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n==== Image File Converter =====\")\n",
        "        print(\"1. Convert Image File\")\n",
        "        print(\"2. OCR (Image → TXT)\")\n",
        "        print(\"3. Exit\")\n",
        "        choice = input(\"Enter choice: \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            img_file = input(\"Enter path of image file (jpg/png/jpeg/gif): \").strip()\n",
        "            print(\"\\nSupported conversions: \")\n",
        "            print(\"1. JPG → PNG\")\n",
        "            print(\"2. PNG → JPG\")\n",
        "            print(\"3. JPG → JPEG\")\n",
        "            print(\"4. PNG → JPEG\")\n",
        "            print(\"5. JPEG → PNG\")\n",
        "            print(\"6. JPEG → JPG\")\n",
        "            print(\"7. GIF → PNG\")\n",
        "            fmt_choice = input(\"Select target format (1-7): \").strip()\n",
        "\n",
        "            base, _ = os.path.splitext(img_file)\n",
        "            out_file = None\n",
        "            try:\n",
        "                if fmt_choice == \"1\": out_file = base + \".png\";  jpg_to_png(img_file, out_file)\n",
        "                elif fmt_choice == \"2\": out_file = base + \".jpg\"; png_to_jpg(img_file, out_file)\n",
        "                elif fmt_choice == \"3\": out_file = base + \".jpeg\"; jpg_to_jpeg(img_file, out_file)\n",
        "                elif fmt_choice == \"4\": out_file = base + \".jpeg\"; png_to_jpeg(img_file, out_file)\n",
        "                elif fmt_choice == \"5\": out_file = base + \".png\";  jpeg_to_png(img_file, out_file)\n",
        "                elif fmt_choice == \"6\": out_file = base + \".jpg\";  jpeg_to_jpg(img_file, out_file)\n",
        "                elif fmt_choice == \"7\": out_file = base + \".png\";  gif_to_png(img_file, out_file)\n",
        "                else:\n",
        "                    print(\"❌ Invalid choice!\")\n",
        "                    continue\n",
        "                print(f\"✅ Converted successfully: {out_file}\")\n",
        "                if files and out_file: files.download(out_file)\n",
        "            except Exception as e:\n",
        "                print(\"❌ Conversion failed:\", e)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            img_file = input(\"Enter path of image: \").strip()\n",
        "            out_file = os.path.splitext(img_file)[0] + \".txt\"\n",
        "            lang_input =  \"auto\"\n",
        "            try:\n",
        "                res = image_to_txt_ocr(img_file, out_file, lang=lang_input) # Pass lang_input here\n",
        "                if res:\n",
        "                    print(f\"✅ OCR done: {res}\")\n",
        "                    if files: files.download(res)\n",
        "            except Exception as e:\n",
        "                print(\"❌ OCR failed:\", e)\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"👋 Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"❌ Invalid choice!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSV Converter Script"
      ],
      "metadata": {
        "id": "4VQuedHNhILl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# csv_converters.py\n",
        "import csv, json\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from docx import Document\n",
        "import os\n",
        "import threading\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import textwrap\n",
        "import math\n",
        "import zipfile\n",
        "import io\n",
        "import unicodedata\n",
        "import shutil # Import shutil for rmtree\n",
        "\n",
        "# ---------- Page setup ----------\n",
        "PAGE_W, PAGE_H = A4\n",
        "\n",
        "# ---------------- Delete after 5 minutes ---------------- #\n",
        "def schedule_delete(file_path, delay=300):  # 300 sec = 5 minutes\n",
        "    try:\n",
        "        def delete_file():\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    if os.path.isdir(file_path):\n",
        "                        import shutil\n",
        "                        shutil.rmtree(file_path)\n",
        "                    else:\n",
        "                        os.remove(file_path)\n",
        "                    print(f\"🗑 Deleted: {file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Failed to delete {file_path}: {e}\")\n",
        "        timer = threading.Timer(delay, delete_file)\n",
        "        timer.start()\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"conversion failed: {e}\")\n",
        "\n",
        "def csv_to_xls(csv_path, xls_path, chunksize=10000):\n",
        "    try:\n",
        "        wb = Workbook(write_only=True)\n",
        "        ws = wb.create_sheet(\"Sheet1\")\n",
        "        first = True\n",
        "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, dtype=str):\n",
        "            if first:\n",
        "                ws.append(list(chunk.columns))\n",
        "                first = False\n",
        "            for row in chunk.itertuples(index=False, name=None):\n",
        "                ws.append(list(row))\n",
        "        wb.save(xls_path)\n",
        "        return xls_path\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ XLS conversion failed: {e}\")\n",
        "\n",
        "def csv_to_pdf(csv_path, pdf_path, margin=40, line_gap=14, font=\"Helvetica\", size=10):\n",
        "    try:\n",
        "        c = canvas.Canvas(pdf_path, pagesize=A4)\n",
        "        c.setFont(font, size)\n",
        "        y = PAGE_H - margin\n",
        "        max_chars = 180\n",
        "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            for row in reader:\n",
        "                line = \" | \".join([str(x) for x in row])\n",
        "                # simple wrap by characters\n",
        "                for chunk in [line[i:i+max_chars] for i in range(0, len(line), max_chars)] or [\" \"]:\n",
        "                    if y < margin:\n",
        "                        c.showPage(); c.setFont(font, size); y = PAGE_H - margin\n",
        "                    c.drawString(margin, y, chunk)\n",
        "                    y -= line_gap\n",
        "        c.save()\n",
        "        return pdf_path\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ PDF conversion failed: {e}\")\n",
        "\n",
        "def csv_to_doc(csv_path, docx_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, dtype=str)\n",
        "        doc = Document()\n",
        "        table = doc.add_table(rows=1, cols=len(df.columns))\n",
        "        for i, col in enumerate(df.columns): table.cell(0,i).text = col\n",
        "        for row in df.itertuples(index=False, name=None):\n",
        "            cells = table.add_row().cells\n",
        "            for i, val in enumerate(row): cells[i].text = \"\" if pd.isna(val) else str(val)\n",
        "        doc.save(docx_path)\n",
        "        return docx_path\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ DOCX conversion failed: {e}\")\n",
        "\n",
        "def csv_to_txt(csv_path, txt_path, delimiter=\"\\t\", chunksize=10000):\n",
        "    try:\n",
        "        first = True\n",
        "        with open(txt_path, \"w\", encoding=\"utf-8\") as out:\n",
        "            for chunk in pd.read_csv(csv_path, dtype=str, chunksize=chunksize):\n",
        "                if first:\n",
        "                    out.write(delimiter.join(chunk.columns) + \"\\n\")\n",
        "                    first = False\n",
        "                for row in chunk.itertuples(index=False, name=None):\n",
        "                    out.write(delimiter.join(\"\" if pd.isna(x) else str(x) for x in row) + \"\\n\")\n",
        "        return txt_path\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ TXT conversion failed: {e}\")\n",
        "\n",
        "def csv_to_json(csv_path, json_path, chunksize=50000):\n",
        "    try:\n",
        "        # Streams rows; writes list-of-objects JSON\n",
        "        first = True\n",
        "        with open(json_path, \"w\", encoding=\"utf-8\") as out:\n",
        "            out.write(\"[\")\n",
        "            for chunk in pd.read_csv(csv_path, dtype=str, chunksize=chunksize):\n",
        "                records = chunk.where(pd.notnull(chunk), None).to_dict(orient=\"records\")\n",
        "                for rec in records:\n",
        "                    if not first: out.write(\",\\n\")\n",
        "                    json.dump(rec, out, ensure_ascii=False)\n",
        "                    first = False\n",
        "            out.write(\"]\")\n",
        "        return json_path\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"❌ TXT conversion failed: {e}\")\n",
        "\n",
        "# ---------- Script to Font mapping (for image conversion) ----------\n",
        "FALLBACK_FONTS = {\n",
        "    \"LATIN\": \"arial.ttf\", # Assuming Arial is available or will be handled\n",
        "    \"DEVANAGARI\": \"NotoSansDevanagari-Medium.ttf\", # Assuming Noto is available or will be handled\n",
        "    \"CJK\": \"arialuni.ttf\", # Arial Unicode MS\n",
        "    \"ARABIC\": \"arial.ttf\",\n",
        "    \"GREEK\": \"arial.ttf\",\n",
        "    \"OTHER\": \"arialuni.ttf\",\n",
        "    \"DEFAULT\": \"arial.ttf\"\n",
        "}\n",
        "\n",
        "def detect_script_simple(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Heuristic script detection based on Unicode codepoints.\n",
        "    Returns one of: DEVANAGARI, CJK, ARABIC, GREEK, HANGUL, LATIN, OTHER\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"LATIN\"\n",
        "    for ch in text:\n",
        "        cp = ord(ch)\n",
        "        if 0x0900 <= cp <= 0x097F:  # Devanagari (Hindi, etc)\n",
        "            return \"DEVANAGARI\"\n",
        "        if (0x4E00 <= cp <= 0x9FFF) or (0x3400 <= cp <= 0x4DBF) or (0x3040 <= cp <= 0x30FF):\n",
        "            return \"CJK\"\n",
        "        if 0xAC00 <= cp <= 0xD7AF:\n",
        "            return \"HANGUL\"\n",
        "        if 0x0600 <= cp <= 0x06FF or 0x0750 <= cp <= 0x077F:\n",
        "            return \"ARABIC\"\n",
        "        if 0x0370 <= cp <= 0x03FF:\n",
        "            return \"GREEK\"\n",
        "    if any(ord(ch) > 127 for ch in text):\n",
        "        return \"OTHER\"\n",
        "    return \"LATIN\"\n",
        "\n",
        "def get_font_path(script, default_size=12):\n",
        "    font_name = FALLBACK_FONTS.get(script, FALLBACK_FONTS[\"DEFAULT\"])\n",
        "    # In a Colab environment, commonly available fonts are in specific paths\n",
        "    # You might need to adjust these paths or download fonts if needed.\n",
        "    # Example paths (may vary):\n",
        "    colab_font_paths = [\n",
        "        f\"/usr/share/fonts/truetype/liberation/{font_name}\", # Liberation fonts often present\n",
        "        f\"/usr/share/fonts/truetype/msttcorefonts/{font_name}\", # If ttf-mscorefonts-installer is run\n",
        "        f\"/usr/share/fonts/truetype/noto/{font_name}\", # Noto fonts\n",
        "        f\"/content/{font_name}\", # If user uploaded\n",
        "        font_name # As a last resort, try just the name\n",
        "    ]\n",
        "\n",
        "    for font_path in colab_font_paths:\n",
        "        if os.path.exists(font_path):\n",
        "            try:\n",
        "                return ImageFont.truetype(font_path, default_size)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not load font {font_path}: {e}\")\n",
        "\n",
        "    # If specific font not found, try a generic fallback font that supports Unicode\n",
        "    try:\n",
        "        # Try a widely available Unicode font like Arial Unicode MS or Noto Sans\n",
        "        fallback_unicode_fonts = [\"arialuni.ttf\", \"NotoSans-Regular.ttf\"]\n",
        "        for fb_font_name in fallback_unicode_fonts:\n",
        "            for fb_path in colab_font_paths:\n",
        "                 potential_path = fb_path.replace(font_name, fb_font_name)\n",
        "                 if os.path.exists(potential_path):\n",
        "                     try:\n",
        "                         return ImageFont.truetype(potential_path, default_size)\n",
        "                     except Exception:\n",
        "                         pass # Try next fallback\n",
        "    except Exception:\n",
        "        pass # Continue to reportlab fallback\n",
        "\n",
        "    # Fallback to ReportLab's default font if Pillow fails\n",
        "    print(f\"⚠️ Font {font_name} not found or could not be loaded. Using default Pillow font.\")\n",
        "    try:\n",
        "        # Attempt to return a basic Pillow font if no TTF found\n",
        "        return ImageFont.load_default()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not load default Pillow font: {e}\")\n",
        "        raise RuntimeError(\"Could not load any font.\")\n",
        "\n",
        "\n",
        "def csv_to_image(csv_path, out_path, font_size=12, margin=40, line_height=18,\n",
        "                 img_width=1200, max_lines_per_img=60, max_safe_height=30000):\n",
        "    \"\"\"\n",
        "    Converts a CSV file to PNG image(s).\n",
        "    Handles large files by splitting into multiple images.\n",
        "    Fix: image height now accounts for wrapped lines (no cropping).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(csv_path):\n",
        "            raise FileNotFoundError(f\"❌ File not found: {csv_path}\")\n",
        "\n",
        "        # Read CSV lines\n",
        "        lines = []\n",
        "        with open(csv_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            for row in reader:\n",
        "                lines.append(\" | \".join(str(x) for x in row))\n",
        "\n",
        "        if not lines:\n",
        "            raise RuntimeError(\"❌ CSV file is empty.\")\n",
        "\n",
        "        total_lines = len(lines)\n",
        "        split_into_multiple = total_lines > max_lines_per_img\n",
        "\n",
        "        # Ask user if too many lines\n",
        "        if split_into_multiple:\n",
        "            print(f\"CSV has {total_lines} lines (>{max_lines_per_img}).\")\n",
        "            print(\"Choose output option:\")\n",
        "            print(\"1. Single giant image (may be very tall)\")\n",
        "            print(\"2. Multiple images (zipped)\")\n",
        "            choice = input(\"Enter choice (1/2): \").strip()\n",
        "            if choice == \"1\":\n",
        "                split_into_multiple = False\n",
        "            else:\n",
        "                split_into_multiple = True\n",
        "\n",
        "        images = []\n",
        "\n",
        "        def render_chunk(chunk, filename):\n",
        "            \"\"\"Render one chunk of CSV into an image file.\"\"\"\n",
        "            # --- Measure wrapped line count first ---\n",
        "            total_wrapped_lines = 0\n",
        "            wrapped_chunks = []\n",
        "            for line in chunk:\n",
        "                script = detect_script_simple(line)\n",
        "                font = get_font_path(script, font_size)\n",
        "                wrapped = textwrap.wrap(line, width=int((img_width - 2*margin) / (font_size*0.6)))\n",
        "                if not wrapped:\n",
        "                    wrapped = [\"\"]\n",
        "                wrapped_chunks.append((wrapped, font))\n",
        "                total_wrapped_lines += len(wrapped)\n",
        "\n",
        "            # --- Now allocate correct height ---\n",
        "            img_height = margin * 2 + total_wrapped_lines * line_height\n",
        "            if img_height > max_safe_height:\n",
        "                print(f\"⚠️ Warning: Image height {img_height}px may be too large.\")\n",
        "            img = Image.new(\"RGB\", (img_width, img_height), \"white\")\n",
        "            draw = ImageDraw.Draw(img)\n",
        "\n",
        "            y_offset = margin\n",
        "            for wrapped, font in wrapped_chunks:\n",
        "                for w_line in wrapped:\n",
        "                    draw.text((margin, y_offset), w_line, fill=\"black\", font=font)\n",
        "                    y_offset += line_height\n",
        "\n",
        "            img.save(filename, \"PNG\")\n",
        "            return filename\n",
        "\n",
        "        if not split_into_multiple:\n",
        "            # --- Option 1: single image ---\n",
        "            if not out_path.lower().endswith(\".png\"):\n",
        "                out_path += \".png\"\n",
        "            images.append(render_chunk(lines, out_path))\n",
        "\n",
        "        else:\n",
        "            # --- Option 2: multiple images zipped ---\n",
        "            chunks = [lines[i:i+max_lines_per_img] for i in range(0, total_lines, max_lines_per_img)]\n",
        "            img_dir = os.path.splitext(out_path)[0] + \"_images\"\n",
        "            os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                img_path = os.path.join(img_dir, f\"page_{i+1}.png\")\n",
        "                images.append(render_chunk(chunk, img_path))\n",
        "\n",
        "            # Zip all pages\n",
        "            zip_path = os.path.splitext(out_path)[0] + \".zip\"\n",
        "            with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "                for img_file in images:\n",
        "                    zf.write(img_file, os.path.basename(img_file))\n",
        "\n",
        "            shutil.rmtree(img_dir)  # cleanup\n",
        "            images = [zip_path]\n",
        "            out_path = zip_path\n",
        "\n",
        "        print(f\"✅ CSV converted to image(s): {out_path}\")\n",
        "        return out_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CSV to Image conversion failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---------------- CLI MENU ---------------- #\n",
        "\n",
        "def main():\n",
        "    # For Colab file handling\n",
        "    try:\n",
        "        from google.colab import files\n",
        "    except ImportError:\n",
        "        files = None\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n==== CSV File Converter =====\")\n",
        "        print(\"1. Convert CSV File\")\n",
        "        print(\"2. Exit\")\n",
        "        choice = input(\"Enter choice: \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            # Upload file (Colab) or enter path (offline)\n",
        "            if files:\n",
        "                #uploaded = files.upload()\n",
        "                #csv_file = list(uploaded.keys())[0]\n",
        "                csv_file = input(\"Enter path of CSV file: \").strip()\n",
        "            else:\n",
        "                csv_file = input(\"Enter path of CSV file: \").strip()\n",
        "\n",
        "\n",
        "            print(\"\\nSupported conversions: \")\n",
        "            print(\"1. CSV → PDF\")\n",
        "            print(\"2. CSV → TXT\")\n",
        "            print(\"3. CSV → PNG (image)\")\n",
        "            print(\"4. CSV → DOCX\")\n",
        "            print(\"5. CSV → XLSX\")\n",
        "            print(\"6. CSV → JSON\")\n",
        "            fmt_choice = input(\"Select target format (1-6): \").strip()\n",
        "\n",
        "            base, _ = os.path.splitext(csv_file)\n",
        "            out_file = None\n",
        "\n",
        "            try:\n",
        "                if fmt_choice == \"1\":\n",
        "                    out_file = base + \".pdf\"\n",
        "                    csv_to_pdf(csv_file, out_file)\n",
        "                elif fmt_choice == \"2\":\n",
        "                    out_file = base + \".txt\"\n",
        "                    csv_to_txt(csv_file, out_file)\n",
        "                elif fmt_choice == \"3\":\n",
        "                    out_file = base + \".png\" # Initial suggestion, might change to .zip\n",
        "                    out_file = csv_to_image(csv_file, out_file) # Update out_file with the actual output path\n",
        "                elif fmt_choice == \"4\":\n",
        "                    out_file = base + \".docx\"\n",
        "                    csv_to_doc(csv_file, out_file)\n",
        "                elif fmt_choice == \"5\":\n",
        "                    out_file = base + \".xlsx\"\n",
        "                    csv_to_xls(csv_file, out_file)\n",
        "                elif fmt_choice == \"6\":\n",
        "                    out_file = base + \".json\"\n",
        "                    csv_to_json(csv_file, out_file)\n",
        "                else:\n",
        "                    print(\"❌ Invalid choice!\")\n",
        "                    continue\n",
        "\n",
        "                if out_file:\n",
        "                     print(f\"✅ Converted successfully: {out_file}\")\n",
        "                     # Schedule auto-delete after 5 minutes\n",
        "                     schedule_delete(out_file)\n",
        "                     # Colab download option\n",
        "                     if files:\n",
        "                        if os.path.isdir(out_file): # if output is a directory (images)\n",
        "                             print(f\"💡 Multiple files saved in {out_file}. You may need to zip and download manually.\")\n",
        "                             # Example of zipping and offering download (requires zip installed)\n",
        "                             # !zip -r {out_file}.zip {out_file}\n",
        "                             # files.download(f\"{out_file}.zip\")\n",
        "                        else:\n",
        "                             files.download(out_file)\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"❌ Conversion failed:\", e)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            print(\"👋 Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"❌ Invalid choice!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "nVEPv97XhM-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XLS Convertion"
      ],
      "metadata": {
        "id": "jUzkKSGDtdzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xls_converters.py\n",
        "import os, sys, csv, json\n",
        "from openpyxl import load_workbook\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from docx import Document\n",
        "import threading # Import threading for schedule_delete\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import textwrap\n",
        "import math\n",
        "import zipfile\n",
        "import io\n",
        "import unicodedata\n",
        "import shutil # Import shutil for rmtree\n",
        "\n",
        "PAGE_W, PAGE_H = A4\n",
        "\n",
        "# ---------- Script to Font mapping (for image conversion) ----------\n",
        "FALLBACK_FONTS = {\n",
        "    \"LATIN\": \"arial.ttf\", # Assuming Arial is available or will be handled\n",
        "    \"DEVANAGARI\": \"NotoSansDevanagari-Medium.ttf\", # Assuming Noto is available or will be handled\n",
        "    \"CJK\": \"arialuni.ttf\", # Arial Unicode MS\n",
        "    \"ARABIC\": \"arial.ttf\",\n",
        "    \"GREEK\": \"arial.ttf\",\n",
        "    \"OTHER\": \"arialuni.ttf\",\n",
        "    \"DEFAULT\": \"arial.ttf\"\n",
        "}\n",
        "\n",
        "def detect_script_simple(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Heuristic script detection based on Unicode codepoints.\n",
        "    Returns one of: DEVANAGARI, CJK, ARABIC, GREEK, HANGUL, LATIN, OTHER\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"LATIN\"\n",
        "    for ch in text:\n",
        "        cp = ord(ch)\n",
        "        if 0x0900 <= cp <= 0x097F:  # Devanagari (Hindi, etc)\n",
        "            return \"DEVANAGARI\"\n",
        "        if (0x4E00 <= cp <= 0x9FFF) or (0x3400 <= cp <= 0x4DBF) or (0x3040 <= cp <= 0x30FF):\n",
        "            return \"CJK\"\n",
        "        if 0xAC00 <= cp <= 0xD7AF:\n",
        "            return \"HANGUL\"\n",
        "        if 0x0600 <= cp <= 0x06FF or 0x0750 <= cp <= 0x077F:\n",
        "            return \"ARABIC\"\n",
        "        if 0x0370 <= cp <= 0x03FF:\n",
        "            return \"GREEK\"\n",
        "    if any(ord(ch) > 127 for ch in text):\n",
        "        return \"OTHER\"\n",
        "    return \"LATIN\"\n",
        "\n",
        "def get_font_path(script, default_size=12):\n",
        "    font_name = FALLBACK_FONTS.get(script, FALLBACK_FONTS[\"DEFAULT\"])\n",
        "    # In a Colab environment, commonly available fonts are in specific paths\n",
        "    # You might need to adjust these paths or download fonts if needed.\n",
        "    # Example paths (may vary):\n",
        "    colab_font_paths = [\n",
        "        f\"/usr/share/fonts/truetype/liberation/{font_name}\", # Liberation fonts often present\n",
        "        f\"/usr/share/fonts/truetype/msttcorefonts/{font_name}\", # If ttf-mscorefonts-installer is run\n",
        "        f\"/usr/share/fonts/truetype/noto/{font_name}\", # Noto fonts\n",
        "        f\"/content/{font_name}\", # If user uploaded\n",
        "        font_name # As a last resort, try just the name\n",
        "    ]\n",
        "\n",
        "    for font_path in colab_font_paths:\n",
        "        if os.path.exists(font_path):\n",
        "            try:\n",
        "                return ImageFont.truetype(font_path, default_size)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not load font {font_path}: {e}\")\n",
        "\n",
        "    # If specific font not found, try a generic fallback font that supports Unicode\n",
        "    try:\n",
        "        # Try a widely available Unicode font like Arial Unicode MS or Noto Sans\n",
        "        fallback_unicode_fonts = [\"arialuni.ttf\", \"NotoSans-Regular.ttf\"]\n",
        "        for fb_font_name in fallback_unicode_fonts:\n",
        "            for fb_path in colab_font_paths:\n",
        "                 potential_path = fb_path.replace(font_name, fb_font_name)\n",
        "                 if os.path.exists(potential_path):\n",
        "                     try:\n",
        "                         return ImageFont.truetype(potential_path, default_size)\n",
        "                     except Exception:\n",
        "                         pass # Try next fallback\n",
        "    except Exception:\n",
        "        pass # Continue to reportlab fallback\n",
        "\n",
        "\n",
        "    # Fallback to ReportLab's default font if Pillow fails\n",
        "    print(f\"⚠️ Font {font_name} not found or could not be loaded. Using default Pillow font.\")\n",
        "    try:\n",
        "        # Attempt to return a basic Pillow font if no TTF found\n",
        "        return ImageFont.load_default()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not load default Pillow font: {e}\")\n",
        "        raise RuntimeError(\"Could not load any font.\")\n",
        "\n",
        "\n",
        "# ---------------- Delete after 5 minutes ---------------- #\n",
        "def schedule_delete(file_path, delay=300):  # 300 sec = 5 minutes\n",
        "    try:\n",
        "        def delete_file():\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    if os.path.isdir(file_path):\n",
        "                        import shutil\n",
        "                        shutil.rmtree(file_path)\n",
        "                    else:\n",
        "                        os.remove(file_path)\n",
        "                    print(f\"🗑 Deleted: {file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Failed to delete {file_path}: {e}\")\n",
        "        timer = threading.Timer(delay, delete_file)\n",
        "        timer.start()\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"conversion failed: {e}\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Core Conversion Functions\n",
        "# -------------------------------\n",
        "\n",
        "def xls_to_csv(xls_path, csv_path, sheet_name=None):\n",
        "    try:\n",
        "        wb = load_workbook(xls_path, read_only=True, data_only=True)\n",
        "        ws = wb[sheet_name] if sheet_name else wb.active\n",
        "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
        "            writer = csv.writer(out)\n",
        "            for row in ws.iter_rows(values_only=True):\n",
        "                writer.writerow([\"\" if v is None else v for v in row])\n",
        "        print(f\"✅ XLS converted to CSV: {csv_path}\")\n",
        "        return csv_path\n",
        "    except Exception as e:\n",
        "        print(f\"❌ XLS to CSV failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def xls_to_doc(xls_path, docx_path, sheet_name=None):\n",
        "    try:\n",
        "        wb = load_workbook(xls_path, read_only=True, data_only=True)\n",
        "        ws = wb[sheet_name] if sheet_name else wb.active\n",
        "        doc = Document()\n",
        "        rows = list(ws.iter_rows(values_only=True))\n",
        "\n",
        "        if not rows:\n",
        "            doc.save(docx_path)\n",
        "            print(f\"⚠️ Empty sheet. Saved empty DOCX: {docx_path}\")\n",
        "            return docx_path\n",
        "\n",
        "        table = doc.add_table(rows=1, cols=len(rows[0]))\n",
        "        for i, val in enumerate(rows[0]):\n",
        "            table.cell(0, i).text = \"\" if val is None else str(val)\n",
        "\n",
        "        for r in rows[1:]:\n",
        "            cells = table.add_row().cells\n",
        "            for i, val in enumerate(r):\n",
        "                cells[i].text = \"\" if val is None else str(val)\n",
        "\n",
        "        doc.save(docx_path)\n",
        "        print(f\"✅ XLS converted to DOCX: {docx_path}\")\n",
        "        return docx_path\n",
        "    except Exception as e:\n",
        "        print(f\"❌ XLS to DOCX failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def xls_to_txt(xls_path, txt_path, delimiter=\"\\t\", sheet_name=None):\n",
        "    try:\n",
        "        wb = load_workbook(xls_path, read_only=True, data_only=True)\n",
        "        ws = wb[sheet_name] if sheet_name else wb.active\n",
        "        with open(txt_path, \"w\", encoding=\"utf-8\") as out:\n",
        "            for row in ws.iter_rows(values_only=True):\n",
        "                out.write(delimiter.join(\"\" if v is None else str(v) for v in row) + \"\\n\")\n",
        "        print(f\"✅ XLS converted to TXT: {txt_path}\")\n",
        "        return txt_path\n",
        "    except Exception as e:\n",
        "        print(f\"❌ XLS to TXT failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def xls_to_pdf(xls_path, pdf_path, margin=40, line_gap=14, font=\"Helvetica\", size=10, sheet_name=None):\n",
        "    try:\n",
        "        wb = load_workbook(xls_path, read_only=True, data_only=True)\n",
        "        ws = wb[sheet_name] if sheet_name else wb.active\n",
        "        c = canvas.Canvas(pdf_path, pagesize=A4)\n",
        "        c.setFont(font, size)\n",
        "\n",
        "        y = PAGE_H - margin\n",
        "        max_chars = 180\n",
        "\n",
        "        for row in ws.iter_rows(values_only=True):\n",
        "            line = \" | \".join(\"\" if v is None else str(v) for v in row)\n",
        "            chunks = [line[i:i+max_chars] for i in range(0, len(line), max_chars)] or [\" \"]\n",
        "            for chunk in chunks:\n",
        "                if y < margin:\n",
        "                    c.showPage()\n",
        "                    c.setFont(font, size)\n",
        "                    y = PAGE_H - margin\n",
        "                c.drawString(margin, y, chunk)\n",
        "                y -= line_gap\n",
        "\n",
        "        c.save()\n",
        "        print(f\"✅ XLS converted to PDF: {pdf_path}\")\n",
        "        return pdf_path\n",
        "    except Exception as e:\n",
        "        print(f\"❌ XLS to PDF failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def xls_to_json(xls_path, json_path, sheet_name=None):\n",
        "    try:\n",
        "        wb = load_workbook(xls_path, read_only=True, data_only=True)\n",
        "        ws = wb[sheet_name] if sheet_name else wb.active\n",
        "        rows = list(ws.iter_rows(values_only=True))\n",
        "\n",
        "        if not rows:\n",
        "            with open(json_path, \"w\", encoding=\"utf-8\") as out:\n",
        "                out.write(\"[]\")\n",
        "            print(f\"⚠️ Empty sheet. Saved empty JSON: {json_path}\")\n",
        "            return json_path\n",
        "\n",
        "        headers = [str(h) if h is not None else \"\" for h in rows[0]]\n",
        "        data = []\n",
        "        for r in rows[1:]:\n",
        "            obj = {headers[i]: (\"\" if r[i] is None else r[i]) for i in range(len(headers))}\n",
        "            data.append(obj)\n",
        "\n",
        "        with open(json_path, \"w\", encoding=\"utf-8\") as out:\n",
        "            json.dump(data, out, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"✅ XLS converted to JSON: {json_path}\")\n",
        "        return json_path\n",
        "    except Exception as e:\n",
        "        print(f\"❌ XLS to JSON failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def xls_to_image(xls_path, out_path, sheet_name=None, font_size=12, margin=40, line_height=18,\n",
        "                 img_width=1200, max_lines_per_img=60, max_safe_height=30000):\n",
        "    \"\"\"\n",
        "    Converts an XLS/XLSX file to PNG image(s).\n",
        "    Handles large files by splitting into multiple images.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(xls_path):\n",
        "            raise FileNotFoundError(f\"❌ File not found: {xls_path}\")\n",
        "\n",
        "        wb = load_workbook(xls_path, read_only=True, data_only=True)\n",
        "        ws = wb[sheet_name] if sheet_name else wb.active\n",
        "\n",
        "        lines = []\n",
        "        for row in ws.iter_rows(values_only=True):\n",
        "            lines.append(\" | \".join(str(x) if x is not None else \"\" for x in row))\n",
        "\n",
        "        if not lines:\n",
        "            raise RuntimeError(\"❌ XLS/XLSX sheet is empty.\")\n",
        "\n",
        "        total_lines = len(lines)\n",
        "        split_into_multiple = total_lines > max_lines_per_img\n",
        "\n",
        "        # Ask user if too many lines\n",
        "        if split_into_multiple:\n",
        "            print(f\"Sheet has {total_lines} lines (>{max_lines_per_img}).\")\n",
        "            print(\"Choose output option:\")\n",
        "            print(\"1. Single giant image (may be very tall)\")\n",
        "            print(\"2. Multiple images (zipped)\")\n",
        "            choice = input(\"Enter choice (1/2): \").strip()\n",
        "            if choice == \"1\":\n",
        "                split_into_multiple = False\n",
        "            else:\n",
        "                split_into_multiple = True\n",
        "\n",
        "        images = []\n",
        "\n",
        "        def render_chunk(chunk, filename):\n",
        "            \"\"\"Render one chunk of XLS data into an image file.\"\"\"\n",
        "            # --- Measure wrapped line count first ---\n",
        "            total_wrapped_lines = 0\n",
        "            wrapped_chunks = []\n",
        "            for line in chunk:\n",
        "                script = detect_script_simple(line)\n",
        "                font = get_font_path(script, font_size)\n",
        "                wrapped = textwrap.wrap(line, width=int((img_width - 2*margin) / (font_size*0.6)))\n",
        "                if not wrapped:\n",
        "                    wrapped = [\"\"]\n",
        "                wrapped_chunks.append((wrapped, font))\n",
        "                total_wrapped_lines += len(wrapped)\n",
        "\n",
        "            # --- Now allocate correct height ---\n",
        "            img_height = margin * 2 + total_wrapped_lines * line_height\n",
        "            if img_height > max_safe_height:\n",
        "                print(f\"⚠️ Warning: Image height {img_height}px may be too large.\")\n",
        "            img = Image.new(\"RGB\", (img_width, img_height), \"white\")\n",
        "            draw = ImageDraw.Draw(img)\n",
        "\n",
        "            y_offset = margin\n",
        "            for wrapped, font in wrapped_chunks:\n",
        "                for w_line in wrapped:\n",
        "                    draw.text((margin, y_offset), w_line, fill=\"black\", font=font)\n",
        "                    y_offset += line_height\n",
        "\n",
        "            img.save(filename, \"PNG\")\n",
        "            return filename\n",
        "\n",
        "\n",
        "        if not split_into_multiple:\n",
        "            # --- Option 1: single image ---\n",
        "            if not out_path.lower().endswith(\".png\"):\n",
        "                out_path += \".png\"\n",
        "            images.append(render_chunk(lines, out_path))\n",
        "\n",
        "        else:\n",
        "            # --- Option 2: multiple images zipped ---\n",
        "            chunks = [lines[i:i+max_lines_per_img] for i in range(0, total_lines, max_lines_per_img)]\n",
        "            img_dir = os.path.splitext(out_path)[0] + \"_images\"\n",
        "            os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                img_path = os.path.join(img_dir, f\"page_{i+1}.png\")\n",
        "                images.append(render_chunk(chunk, img_path))\n",
        "\n",
        "            # Zip all pages\n",
        "            zip_path = os.path.splitext(out_path)[0] + \".zip\"\n",
        "            with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "                for img_file in images:\n",
        "                    zf.write(img_file, os.path.basename(img_file))\n",
        "\n",
        "            shutil.rmtree(img_dir)  # cleanup\n",
        "            images = [zip_path]\n",
        "            out_path = zip_path\n",
        "\n",
        "\n",
        "        print(f\"✅ XLS/XLSX converted to image(s): {out_path}\")\n",
        "        return out_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ XLS/XLSX to Image conversion failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# CLI Tool (Modified for Colab)\n",
        "# -------------------------------\n",
        "def main():\n",
        "    # For Colab file handling\n",
        "    try:\n",
        "        from google.colab import files\n",
        "    except ImportError:\n",
        "        files = None\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n==== XLS File Converter =====\")\n",
        "        print(\"1. Convert XLS/XLSX File\")\n",
        "        print(\"2. Exit\")\n",
        "        choice = input(\"Enter choice: \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            # Upload file (Colab) or enter path (offline)\n",
        "            if files:\n",
        "                #uploaded = files.upload()\n",
        "                #xls_file = list(uploaded.keys())[0]\n",
        "                xls_file = input(\"Enter path of XLS/XLSX file: \").strip()\n",
        "            else:\n",
        "                xls_file = input(\"Enter path of XLS/XLSX file: \").strip()\n",
        "\n",
        "            print(\"\\nSupported conversions: \")\n",
        "            print(\"1. XLS → CSV\")\n",
        "            print(\"2. XLS → TXT\")\n",
        "            print(\"3. XLS → PDF\")\n",
        "            print(\"4. XLS → DOCX\")\n",
        "            print(\"5. XLS → JSON\")\n",
        "            print(\"6. XLS → IMAGE(PNG)\") # Added Image option\n",
        "            fmt_choice = input(\"Select target format (1-6): \").strip()\n",
        "\n",
        "            base, _ = os.path.splitext(xls_file)\n",
        "            out_file = None\n",
        "            sheet_name = None # You might want to add an option to select sheet name\n",
        "\n",
        "            try:\n",
        "                if fmt_choice == \"1\":\n",
        "                    out_file = base + \".csv\"\n",
        "                    xls_to_csv(xls_file, out_file, sheet_name)\n",
        "                elif fmt_choice == \"2\":\n",
        "                    out_file = base + \".txt\"\n",
        "                    xls_to_txt(xls_file, out_file, sheet_name=sheet_name)\n",
        "                elif fmt_choice == \"3\":\n",
        "                    out_file = base + \".pdf\"\n",
        "                    xls_to_pdf(xls_file, out_file, sheet_name=sheet_name)\n",
        "                elif fmt_choice == \"4\":\n",
        "                    out_file = base + \".docx\"\n",
        "                    xls_to_doc(xls_file, out_file, sheet_name)\n",
        "                elif fmt_choice == \"5\":\n",
        "                    out_file = base + \".json\"\n",
        "                    xls_to_json(xls_file, out_file, sheet_name)\n",
        "                elif fmt_choice == \"6\":\n",
        "                    out_file = base + \".png\" # Initial suggestion, might change to .zip\n",
        "                    out_file = xls_to_image(xls_file, out_file, sheet_name=sheet_name)\n",
        "                else:\n",
        "                    print(\"❌ Invalid choice!\")\n",
        "                    continue\n",
        "\n",
        "                if out_file:\n",
        "                     print(f\"✅ Converted successfully: {out_file}\")\n",
        "                     # Schedule auto-delete after 5 minutes\n",
        "                     schedule_delete(out_file)\n",
        "                     # Colab download option\n",
        "                     if files:\n",
        "                        if os.path.isdir(out_file): # if output is a directory (images)\n",
        "                             print(f\"💡 Multiple files saved in {out_file}. You may need to zip and download manually.\")\n",
        "                        else:\n",
        "                             files.download(out_file)\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"❌ Conversion failed:\", e)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            print(\"👋 Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"❌ Invalid choice!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mzjjRUA1thtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete Code"
      ],
      "metadata": {
        "id": "rdqE2H5pyVe-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EkyFBM1TyZHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "597210d5"
      },
      "source": [
        "Here are test cases for the individual conversion functions using `pytest`.\n",
        "\n",
        "**Note:**\n",
        "1. You need to have `pytest` installed (`!pip install pytest`).\n",
        "2. These tests require dummy input files (e.g., `dummy.csv`, `dummy.xlsx`, `dummy.txt`, `dummy.docx`, `dummy.pdf`, `dummy.json`, `dummy.png`) in the same directory as the script or provide the correct paths.\n",
        "3. Some tests might require external dependencies like LibreOffice (`soffice`) for `.doc` to `.docx` conversion, or specific fonts.\n",
        "4. The image conversion tests might be sensitive to font availability and exact rendering, so they might need adjustments based on your environment.\n",
        "5. The `schedule_delete` function is mocked to prevent actual file deletion during tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc5ac67a"
      },
      "source": [
        "import pytest\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from unittest.mock import patch, MagicMock # Import MagicMock\n",
        "\n",
        "# Assuming the converter functions are in the current notebook cell (or imported)\n",
        "\n",
        "# --- Helper to create dummy files ---\n",
        "def create_dummy_csv(filename=\"dummy.csv\"):\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"col1\", \"col2\", \"col3\"])\n",
        "        writer.writerow([\"data1\", \"data2\", \"data3\"])\n",
        "        writer.writerow([\"value 😅\", \"हिंदी\", \"another,value\"])\n",
        "    return filename\n",
        "\n",
        "def create_dummy_xlsx(filename=\"dummy.xlsx\"):\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    ws.append([\"colA\", \"colB\", \"colC\"])\n",
        "    ws.append([\"dataA\", \"dataB\", \"dataC\"])\n",
        "    ws.append([\"value 😊\", \"मराठी\", \"one|two|three\"])\n",
        "    wb.save(filename)\n",
        "    return filename\n",
        "\n",
        "def create_dummy_txt(filename=\"dummy.txt\"):\n",
        "    content = \"\"\"\n",
        "This is a test text file.\n",
        "It has multiple lines.\n",
        "Including some Unicode characters: नमस्ते\n",
        "And a blank line above.\n",
        "\"\"\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "    return filename\n",
        "\n",
        "def create_dummy_docx(filename=\"dummy.docx\"):\n",
        "    doc = Document()\n",
        "    doc.add_paragraph(\"This is a test document.\")\n",
        "    doc.add_paragraph(\"It also has text in Hindi: नमस्ते\")\n",
        "    table = doc.add_table(rows=2, cols=3)\n",
        "    table.cell(0, 0).text = \"Header 1\"\n",
        "    table.cell(0, 1).text = \"Header 2\"\n",
        "    table.cell(0, 2).text = \"Header 3\"\n",
        "    table.cell(1, 0).text = \"Row 1 Data 1\"\n",
        "    table.cell(1, 1).text = \"Row 1 Data 2 😊\"\n",
        "    table.cell(1, 2).text = \"रो 1 डेटा 3\"\n",
        "    doc.save(filename)\n",
        "    return filename\n",
        "\n",
        "def create_dummy_pdf(filename=\"dummy.pdf\"):\n",
        "    # Using reportlab to create a simple PDF\n",
        "    c = canvas.Canvas(filename, pagesize=A4)\n",
        "    c.drawString(100, 750, \"Test PDF Content\")\n",
        "    c.drawString(100, 735, \"With some Hindi: नमस्ते\")\n",
        "    # Add a simple \"table\" like structure\n",
        "    c.drawString(100, 700, \"Col1 | Col2 | Col3\")\n",
        "    c.drawString(100, 685, \"Data1 | Data2 | Data3\")\n",
        "    c.save()\n",
        "    return filename\n",
        "\n",
        "def create_dummy_json(filename=\"dummy.json\"):\n",
        "    data = [\n",
        "        {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
        "        {\"name\": \"Bob\", \"age\": 25, \"city\": \"London\", \"greeting\": \"नमस्ते\"}\n",
        "    ]\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "    return filename\n",
        "\n",
        "def create_dummy_image(filename=\"dummy.png\"):\n",
        "    img = Image.new('RGB', (100, 50), color = (255, 255, 255))\n",
        "    d = ImageDraw.Draw(img)\n",
        "    # Use a font that supports Unicode if possible\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/noto/NotoSans-Regular.ttf\", 15)\n",
        "    except Exception:\n",
        "         font = ImageFont.load_default()\n",
        "    d.text((10,10), \"Test Image नमस्ते\", fill=(0,0,0), font=font)\n",
        "    img.save(filename)\n",
        "    return filename\n",
        "\n",
        "# --- Fixture for temporary directory ---\n",
        "@pytest.fixture\n",
        "def temp_dir(request):\n",
        "    tmpdir = tempfile.mkdtemp()\n",
        "    def cleanup():\n",
        "        shutil.rmtree(tmpdir)\n",
        "    request.addfinalizer(cleanup)\n",
        "    return tmpdir\n",
        "\n",
        "# --- Mock the schedule_delete function ---\n",
        "@pytest.fixture(autouse=True)\n",
        "def mock_schedule_delete():\n",
        "    with patch('__main__.schedule_delete') as mock_delete: # Adjust '__main__' if importing\n",
        "        yield mock_delete\n",
        "\n",
        "# --- Test Cases ---\n",
        "\n",
        "# CSV Conversions\n",
        "def test_csv_to_xls(temp_dir):\n",
        "    csv_file = create_dummy_csv(os.path.join(temp_dir, \"test.csv\"))\n",
        "    xls_file = os.path.join(temp_dir, \"output.xlsx\")\n",
        "    result = csv_to_xls(csv_file, xls_file)\n",
        "    assert os.path.exists(result)\n",
        "    assert os.path.getsize(result) > 0\n",
        "    # Basic check on content (optional, requires openpyxl)\n",
        "    wb = load_workbook(result)\n",
        "    ws = wb.active\n",
        "    rows = list(ws.iter_rows(values_only=True))\n",
        "    assert len(rows) == 3\n",
        "    assert rows[0] == (\"col1\", \"col2\", \"col3\")\n",
        "\n",
        "def test_csv_to_pdf(temp_dir):\n",
        "    csv_file = create_dummy_csv(os.path.join(temp_dir, \"test.csv\"))\n",
        "    pdf_file = os.path.join(temp_dir, \"output.pdf\")\n",
        "    result = csv_to_pdf(csv_file, pdf_file)\n",
        "    assert os.path.exists(result)\n",
        "    assert os.path.getsize(result) > 0\n",
        "\n",
        "def test_csv_to_doc(temp_dir):\n",
        "    csv_file = create_dummy_csv(os.path.join(temp_dir, \"test.csv\"))\n",
        "    docx_file = os.path.join(temp_dir, \"output.docx\")\n",
        "    result = csv_to_doc(csv_file, docx_file)\n",
        "    assert os.path.exists(result)\n",
        "    assert os.path.getsize(result) > 0\n",
        "    # Basic check on content (optional, requires python-docx)\n",
        "    # doc = Document(result)\n",
        "    # assert len(doc.tables) > 0\n",
        "\n",
        "def test_csv_to_txt(temp_dir):\n",
        "    csv_file = create_dummy_csv(os.path.join(temp_dir, \"test.csv\"))\n",
        "    txt_file = os.path.join(temp_dir, \"output.txt\")\n",
        "    result = csv_to_txt(csv_file, txt_file, delimiter=\",\") # Test with comma delimiter\n",
        "    assert os.path.exists(result)\n",
        "    with open(result, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "    assert \"col1,col2,col3\" in content\n",
        "    assert \"value 😅,हिंदी,another,value\" in content # Check for delimiter and content\n",
        "\n",
        "def test_csv_to_json(temp_dir):\n",
        "    csv_file = create_dummy_csv(os.path.join(temp_dir, \"test.csv\"))\n",
        "    json_file = os.path.join(temp_dir, \"output.json\")\n",
        "    result = csv_to_json(csv_file, json_file)\n",
        "    assert os.path.exists(result)\n",
        "    with open(result, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    assert isinstance(data, list)\n",
        "    assert len(data) == 2 # Header row is not included in data\n",
        "    assert data[0][\"col1\"] == \"data1\"\n",
        "    assert data[1][\"col2\"] == \"हिंदी\"\n",
        "\n",
        "# XLS Conversions\n",
        "def test_xls_to_csv(temp_dir):\n",
        "    xls_file = create_dummy_xlsx(os.path.join(temp_dir, \"test.xlsx\"))\n",
        "    csv_file = os.path.join(temp_dir, \"output.csv\")\n",
        "    result = xls_to_csv(xls_file, csv_file)\n",
        "    assert os.path.exists(result)\n",
        "    assert os.path.getsize(result) > 0\n",
        "    with open(result, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "    assert \"colA,colB,colC\" in content\n",
        "    assert \"value 😊,मराठी,one|two|three\" in content\n",
        "\n",
        "def test_xls_to_doc(temp_dir):\n",
        "    xls_file = create_dummy_xlsx(os.path.join(temp_dir, \"test.xlsx\"))\n",
        "    docx_file = os.path.join(temp_dir, \"output.docx\")\n",
        "    result = xls_to_doc(xls_file, docx_file)\n",
        "    assert os.path.exists(result)\n",
        "    assert os.path.getsize(result) > 0\n",
        "\n",
        "def test_xls_to_txt(temp_dir):\n",
        "    xls_file = create_dummy_xlsx(os.path.join(temp_dir, \"test.xlsx\"))\n",
        "    txt_file = os.path.join(temp_dir, \"output.txt\")\n",
        "    result = xls_to_txt(xls_file, txt_file, delimiter=\"|\") # Test with pipe delimiter\n",
        "    assert os.path.exists(result)\n",
        "    with open(result, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "    assert \"colA|colB|colC\" in content\n",
        "    assert \"value 😊|मराठी|one|two|three\" in content\n",
        "\n",
        "def test_xls_to_pdf(temp_dir):\n",
        "    xls_file = create_dummy_xlsx(os.path.join(temp_dir, \"test.xlsx\"))\n",
        "    pdf_file = os.path.join(temp_dir, \"output.pdf\")\n",
        "    result = xls_to_pdf(xls_file, pdf_file)\n",
        "    assert os.path.exists(result)\n",
        "    assert os.path.getsize(result) > 0\n",
        "\n",
        "def test_xls_to_json(temp_dir):\n",
        "    xls_file = create_dummy_xlsx(os.path.join(temp_dir, \"test.xlsx\"))\n",
        "    json_file = os.path.join(temp_dir, \"output.json\")\n",
        "    result = xls_to_json(xls_file, json_file)\n",
        "    assert os.path.exists(result)\n",
        "    with open(result, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    assert isinstance(data, list)\n",
        "    assert len(data) == 2 # Header row is not included in data\n",
        "    assert data[0][\"colA\"] == \"dataA\"\n",
        "    assert data[1][\"colB\"] == \"मराठी\"\n",
        "\n",
        "# Image Conversions (Basic tests, OCR accuracy varies)\n",
        "@patch('__main__._get_easyocr', return_value=MagicMock()) # Mock EasyOCR initialization\n",
        "@patch('pytesseract.image_to_string') # Mock Tesseract\n",
        "def test_image_to_txt_ocr(mock_tesseract, mock_easyocr_reader, temp_dir):\n",
        "    # Configure mocks\n",
        "    mock_easyocr_reader.return_value.readtext.return_value = [\"Mocked EasyOCR Text नमस्ते\"]\n",
        "    mock_tesseract.return_value = \"Mocked Tesseract Text हिंदी\"\n",
        "\n",
        "    img_file = create_dummy_image(os.path.join(temp_dir, \"test.png\"))\n",
        "    txt_file = os.path.join(temp_dir, \"output.txt\")\n",
        "\n",
        "    # Test with EasyOCR (if enabled)\n",
        "    result = image_to_txt_ocr(img_file, txt_file, lang='en,hi')\n",
        "    assert os.path.exists(result)\n",
        "    with open(result, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "    # Check if either engine's output is in the file\n",
        "    assert \"Mocked EasyOCR Text नमस्ते\" in content or \"Mocked Tesseract Text हिंदी\" in content\n",
        "\n",
        "    # Test fallback to Tesseract if EasyOCR fails (simulate EasyOCR failure)\n",
        "    mock_easyocr_reader.return_value.readtext.side_effect = Exception(\"EasyOCR Error\")\n",
        "    result = image_to_txt_ocr(img_file, txt_file, lang='en,hi')\n",
        "    assert os.path.exists(result)\n",
        "    with open(result, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "    assert \"Mocked Tesseract Text हिंदी\" in content # Should contain Tesseract output\n",
        "\n",
        "def test_image_to_image(temp_dir):\n",
        "    img_file = create_dummy_image(os.path.join(temp_dir, \"test.png\"))\n",
        "    jpg_file = os.path.join(temp_dir, \"output.jpg\")\n",
        "    result = image_to_image(img_file, jpg_file, \"JPEG\")\n",
        "    assert os.path.exists(result)\n",
        "    assert os.path.getsize(result) > 0\n",
        "    assert result.lower().endswith(\".jpg\")\n",
        "\n",
        "# TXT Conversions\n",
        "def test_txt_to_pdf(temp_dir):\n",
        "    txt_file = create_dummy_txt(os.path.join(temp_dir, \"test.txt\"))\n",
        "    pdf_file = os.path.join(temp_dir, \"output.pdf\")\n",
        "    result = txt_to_pdf(txt_file, pdf_file)\n",
        "    assert os.path.exists(result)\n",
        "    assert os.path.getsize(result) > 0\n",
        "\n",
        "@patch('__main__.Document') # Mock Document to avoid actual file creation/dependencies\n",
        "def test_txt_to_doc(mock_document, temp_dir):\n",
        "    txt_file = create_dummy_txt(os.path.join(temp_dir, \"test.txt\"))\n",
        "    docx_file = os.path.join(temp_dir, \"output.docx\")\n",
        "    # Configure the mock Document object\n",
        "    mock_doc_instance = MagicMock()\n",
        "    mock_document.return_value = mock_doc_instance\n",
        "\n",
        "    result = txt_to_doc(txt_file, docx_file)\n",
        "\n",
        "    # Assert that Document was called and save was called\n",
        "    mock_document.assert_called_once()\n",
        "    mock_doc_instance.save.assert_called_once_with(docx_file)\n",
        "    # We can't easily assert file content without actual docx creation\n",
        "\n",
        "def test_txt_to_json(temp_dir):\n",
        "    txt_file = create_dummy_txt(os.path.join(temp_dir, \"test.txt\"))\n",
        "    json_file = os.path.join(temp_dir, \"output.json\")\n",
        "    result = txt_to_json(txt_file, json_file)\n",
        "    assert os.path.exists(result)\n",
        "    with open(result, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    assert isinstance(data, list)\n",
        "    assert \"This is a test text file.\" in data\n",
        "\n",
        "# DOC/DOCX Conversions (Require LibreOffice or heavy mocking)\n",
        "@patch('__main__.convert_doc_to_docx_if_needed', return_value='mocked_input.docx')\n",
        "@patch('__main__.convert_docx_to_pdf_libreoffice')\n",
        "@patch('__main__.mammoth.convert_to_html')\n",
        "@patch('__main__.pdfkit.from_file')\n",
        "def test_doc_to_pdf_auto(mock_pdfkit, mock_mammoth, mock_libreoffice_pdf, mock_doc_to_docx, temp_dir):\n",
        "    dummy_doc = os.path.join(temp_dir, \"test.doc\")\n",
        "    output_pdf = os.path.join(temp_dir, \"output.pdf\")\n",
        "\n",
        "    # Mock docx_path to exist for subsequent steps\n",
        "    mock_doc_to_docx.return_value = os.path.join(temp_dir, \"mocked_input.docx\")\n",
        "    # Create a dummy mocked docx file for the table check\n",
        "    mock_docx_file = os.path.join(temp_dir, \"mocked_input.docx\")\n",
        "    # We need a minimal docx file to avoid errors when python-docx tries to open it\n",
        "    # A simple empty docx is enough if we mock the table check later\n",
        "    Document().save(mock_docx_file)\n",
        "\n",
        "\n",
        "    # Mock the table inspection within doc_to_pdf\n",
        "    with patch('__main__.Document') as mock_Document_for_table_check:\n",
        "        mock_doc_instance = MagicMock()\n",
        "        mock_Document_for_table_check.return_value = mock_doc_instance\n",
        "\n",
        "        # Case 1: No wide tables -> LibreOffice path\n",
        "        mock_doc_instance.tables = [] # No tables detected\n",
        "        mock_libreoffice_pdf.return_value = os.path.join(temp_dir, \"libreoffice_output.pdf\")\n",
        "        # Create the mocked LibreOffice output file\n",
        "        with open(os.path.join(temp_dir, \"libreoffice_output.pdf\"), 'w') as f:\n",
        "            f.write(\"mock pdf content\")\n",
        "\n",
        "        result = doc_to_pdf(dummy_doc, output_pdf, mode=\"auto\")\n",
        "        mock_libreoffice_pdf.assert_called_once()\n",
        "        assert os.path.exists(output_pdf)\n",
        "        # Check if the content is from the mocked LibreOffice PDF\n",
        "        with open(output_pdf, 'r') as f:\n",
        "            assert \"mock pdf content\" in f.read()\n",
        "\n",
        "        # Reset mocks for next case\n",
        "        mock_libreoffice_pdf.reset_mock()\n",
        "        mock_mammoth.reset_mock()\n",
        "        mock_pdfkit.reset_mock()\n",
        "        os.remove(output_pdf) # Clean up output\n",
        "\n",
        "        # Case 2: Wide tables -> HTML path\n",
        "        mock_doc_instance = MagicMock()\n",
        "        mock_Document_for_table_check.return_value = mock_doc_instance\n",
        "        # Simulate a wide table\n",
        "        mock_table = MagicMock()\n",
        "        mock_table.rows = [MagicMock()] # Need at least one row\n",
        "        mock_table.rows[0].cells = [MagicMock()] * 5 # 5 columns\n",
        "        mock_doc_instance.tables = [mock_table]\n",
        "\n",
        "        mock_mammoth.return_value.value = \"<html><body>Mock HTML Content</body></html>\"\n",
        "        mock_pdfkit.return_value = True # pdfkit returns True on success\n",
        "\n",
        "        result = doc_to_pdf(dummy_doc, output_pdf, mode=\"auto\")\n",
        "        mock_mammoth.assert_called_once()\n",
        "        mock_pdfkit.assert_called_once()\n",
        "        # We can't easily check the final PDF content here as it's mocked\n",
        "\n",
        "@patch('__main__.convert_doc_to_docx_if_needed', return_value='mocked_input.docx')\n",
        "@patch('__main__.convert_docx_to_pdf_libreoffice')\n",
        "@patch('__main__.convert_from_path')\n",
        "@patch('PIL.Image.Image.save')\n",
        "def test_doc_to_pdf_image(mock_image_save, mock_convert_from_path, mock_libreoffice_pdf, mock_doc_to_docx, temp_dir):\n",
        "    dummy_doc = os.path.join(temp_dir, \"test.doc\")\n",
        "    output_pdf = os.path.join(temp_dir, \"output.pdf\")\n",
        "\n",
        "    # Mock docx_path to exist\n",
        "    mock_doc_to_docx.return_value = os.path.join(temp_dir, \"mocked_input.docx\")\n",
        "\n",
        "    # Mock LibreOffice PDF creation\n",
        "    mock_libreoffice_pdf.return_value = os.path.join(temp_dir, \"libreoffice_output.pdf\")\n",
        "    # Create the mocked LibreOffice output file\n",
        "    with open(os.path.join(temp_dir, \"libreoffice_output.pdf\"), 'w') as f:\n",
        "        f.write(\"mock pdf content\")\n",
        "\n",
        "    # Mock pdf2image conversion\n",
        "    mock_image_instance = MagicMock()\n",
        "    mock_convert_from_path.return_value = [mock_image_instance] # Simulate one page\n",
        "\n",
        "    result = doc_to_pdf(dummy_doc, output_pdf, mode=\"image\")\n",
        "\n",
        "    mock_doc_to_docx.assert_called_once_with(dummy_doc)\n",
        "    mock_libreoffice_pdf.assert_called_once()\n",
        "    mock_convert_from_path.assert_called_once()\n",
        "    mock_image_save.assert_called_once_with(output_pdf, save_all=True, append_images=[])\n",
        "    assert os.path.exists(output_pdf) # Check if the final output file was created\n",
        "\n",
        "# PDF Conversions\n",
        "@patch('pdfplumber.open')\n",
        "def test_pdf_to_txt(mock_pdfplumber_open, temp_dir):\n",
        "    dummy_pdf = os.path.join(temp_dir, \"test.pdf\")\n",
        "    output_txt = os.path.join(temp_dir, \"output.txt\")\n",
        "\n",
        "    # Mock pdfplumber\n",
        "    mock_pdf = MagicMock()\n",
        "    mock_page1 = MagicMock()\n",
        "    mock_page1.extract_text.return_value = \"Page 1 Text नमस्ते\"\n",
        "    mock_page2 = MagicMock()\n",
        "    mock_page2.extract_text.return_value = \"Page 2 Text हिंदी\"\n",
        "    mock_pdf.pages = [mock_page1, mock_page2]\n",
        "    mock_pdfplumber_open.return_value.__enter__.return_value = mock_pdf\n",
        "\n",
        "    # Create a dummy input file for os.path.exists check (content doesn't matter)\n",
        "    with open(dummy_pdf, 'w') as f: f.write(\"dummy\")\n",
        "\n",
        "    result = pdf_to_txt(dummy_pdf, output_txt)\n",
        "\n",
        "    mock_pdfplumber_open.assert_called_once_with(dummy_pdf)\n",
        "    mock_page1.extract_text.assert_called_once()\n",
        "    mock_page2.extract_text.assert_called_once()\n",
        "    assert os.path.exists(result)\n",
        "    with open(result, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    assert \"Page 1 Text नमस्ते\" in content\n",
        "    assert \"Page 2 Text हिंदी\" in content\n",
        "\n",
        "@patch('pdfplumber.open')\n",
        "@patch('__main__.Document')\n",
        "def test_pdf_to_doc(mock_document, mock_pdfplumber_open, temp_dir):\n",
        "    dummy_pdf = os.path.join(temp_dir, \"test.pdf\")\n",
        "    output_docx = os.path.join(temp_dir, \"output.docx\")\n",
        "\n",
        "    # Mock pdfplumber\n",
        "    mock_pdf = MagicMock()\n",
        "    mock_page1 = MagicMock()\n",
        "    mock_page1.extract_text.return_value = \"Page 1 Text नमस्ते\"\n",
        "    mock_page1.images = [] # No images for this test\n",
        "    mock_page2 = MagicMock()\n",
        "    mock_page2.extract_text.return_value = \"Page 2 Text हिंदी\"\n",
        "    mock_page2.images = []\n",
        "    mock_pdf.pages = [mock_page1, mock_page2]\n",
        "    mock_pdfplumber_open.return_value.__enter__.return_value = mock_pdf\n",
        "    mock_pdfplumber_open.return_value.__exit__.return_value = None # Mock context manager exit\n",
        "\n",
        "    # Mock python-docx Document\n",
        "    mock_doc_instance = MagicMock()\n",
        "    mock_document.return_value = mock_doc_instance\n",
        "\n",
        "    # Create a dummy input file for os.path.exists check\n",
        "    with open(dummy_pdf, 'w') as f: f.write(\"dummy\")\n",
        "\n",
        "    result = pdf_to_doc(dummy_pdf, output_docx)\n",
        "\n",
        "    mock_pdfplumber_open.assert_called_once_with(dummy_pdf)\n",
        "    mock_document.assert_called_once()\n",
        "    # Check if add_paragraph was called for each line (roughly)\n",
        "    assert mock_doc_instance.add_paragraph.call_count >= 2 # At least one paragraph per page\n",
        "    mock_doc_instance.save.assert_called_once_with(output_docx)\n",
        "    # We can't easily assert file content without actual docx creation\n",
        "\n",
        "@patch('pdf2image.convert_from_path')\n",
        "@patch('PIL.Image.Image.save')\n",
        "@patch('builtins.input', return_value='2') # Mock user input for multi-page choice\n",
        "@patch('shutil.rmtree') # Mock rmtree to prevent cleanup issues\n",
        "def test_pdf_to_image_multi_page_zip(mock_rmtree, mock_input, mock_image_save, mock_convert_from_path, temp_dir):\n",
        "    dummy_pdf = os.path.join(temp_dir, \"test.pdf\")\n",
        "    output_dir = os.path.join(temp_dir, \"output_images\")\n",
        "\n",
        "    # Mock pdf2image to return multiple images\n",
        "    mock_img1 = MagicMock()\n",
        "    mock_img1.size = (100, 100)\n",
        "    mock_img2 = MagicMock()\n",
        "    mock_img2.size = (100, 100)\n",
        "    mock_convert_from_path.return_value = [mock_img1, mock_img2]\n",
        "\n",
        "    # Create a dummy input file\n",
        "    with open(dummy_pdf, 'w') as f: f.write(\"dummy\")\n",
        "\n",
        "    # Mock zipfile.ZipFile to prevent actual zipping during the test\n",
        "    with patch('zipfile.ZipFile') as mock_zipfile:\n",
        "        mock_zip_instance = MagicMock()\n",
        "        mock_zipfile.return_value.__enter__.return_value = mock_zip_instance\n",
        "\n",
        "        result = pdf_to_image(dummy_pdf, output_dir, fmt=\"png\")\n",
        "\n",
        "        mock_convert_from_path.assert_called_once_with(dummy_pdf, dpi=150)\n",
        "        # Check if save was called for each image\n",
        "        assert mock_image_save.call_count == 2\n",
        "        # Check if zipfile was called\n",
        "        mock_zipfile.assert_called_once()\n",
        "        # Check if rmtree was called for cleanup\n",
        "        mock_rmtree.assert_called_once_with(output_dir)\n",
        "        # Check if the result is the zip path (adjust if your function returns dir path)\n",
        "        # Based on the code, it returns the directory path, which is then zipped in main.\n",
        "        # Let's check for the existence of the output directory instead for this mock test.\n",
        "        assert os.path.exists(output_dir) # Directory is created before rmtree mock\n",
        "\n",
        "@patch('pdfplumber.open')\n",
        "@patch('csv.writer')\n",
        "@patch('openpyxl.Workbook')\n",
        "def test_pdf_to_csv_xlsx(mock_workbook, mock_csv_writer, mock_pdfplumber_open, temp_dir):\n",
        "    dummy_pdf = os.path.join(temp_dir, \"test.pdf\")\n",
        "    output_csv = os.path.join(temp_dir, \"output.csv\")\n",
        "    output_xlsx = os.path.join(temp_dir, \"output.xlsx\")\n",
        "\n",
        "    # Mock pdfplumber\n",
        "    mock_pdf = MagicMock()\n",
        "    mock_page1 = MagicMock()\n",
        "    # Simulate a table with rows\n",
        "    mock_page1.extract_tables.return_value = [[[\"Header1\", \"Header2\"], [\"Data1\", \"Data2\"]]]\n",
        "    mock_page1.extract_text.return_value = \"\" # No plain text\n",
        "    mock_pdf.pages = [mock_page1]\n",
        "    mock_pdfplumber_open.return_value.__enter__.return_value = mock_pdf\n",
        "    mock_pdfplumber_open.return_value.__exit__.return_value = None # Mock context manager exit\n",
        "\n",
        "    # Mock csv writer\n",
        "    mock_csv_writer_instance = MagicMock()\n",
        "    mock_csv_writer.return_value = mock_csv_writer_instance\n",
        "\n",
        "    # Mock openpyxl Workbook\n",
        "    mock_wb_instance = MagicMock()\n",
        "    mock_ws_instance = MagicMock()\n",
        "    mock_workbook.return_value = mock_wb_instance\n",
        "    mock_wb_instance.create_sheet.return_value = mock_ws_instance\n",
        "\n",
        "    # Create dummy input file\n",
        "    with open(dummy_pdf, 'w') as f: f.write(\"dummy\")\n",
        "\n",
        "    result = pdf_to_csv(dummy_pdf, csv_path=output_csv, xlsx_path=output_xlsx)\n",
        "\n",
        "    mock_pdfplumber_open.assert_called_once_with(dummy_pdf)\n",
        "    mock_csv_writer.assert_called_once()\n",
        "    mock_workbook.assert_called_once()\n",
        "    # Check if rows were written to CSV and XLSX\n",
        "    assert mock_csv_writer_instance.writerow.call_count == 2 # Header + Data\n",
        "    assert mock_ws_instance.append.call_count == 2 # Header + Data\n",
        "    mock_wb_instance.save.assert_called_once_with(output_xlsx)\n",
        "    assert os.path.exists(output_csv)\n",
        "    assert os.path.exists(output_xlsx)\n",
        "\n",
        "# JSON Conversions\n",
        "@patch('pdfkit.from_file')\n",
        "def test_json_to_pdf(mock_pdfkit_from_file, temp_dir):\n",
        "    json_file = create_dummy_json(os.path.join(temp_dir, \"test.json\"))\n",
        "    pdf_file = os.path.join(temp_dir, \"output.pdf\")\n",
        "\n",
        "    mock_pdfkit_from_file.return_value = True # Simulate success\n",
        "\n",
        "    result = json_to_pdf(json_file, pdf_file)\n",
        "\n",
        "    mock_pdfkit_from_file.assert_called_once()\n",
        "    assert result == pdf_file # Assuming success returns the output path\n",
        "\n",
        "@patch('__main__.Document')\n",
        "def test_json_to_doc(mock_document, temp_dir):\n",
        "    json_file = create_dummy_json(os.path.join(temp_dir, \"test.json\"))\n",
        "    docx_file = os.path.join(temp_dir, \"output.docx\")\n",
        "\n",
        "    mock_doc_instance = MagicMock()\n",
        "    mock_document.return_value = mock_doc_instance\n",
        "\n",
        "    result = json_to_doc(json_file, docx_file)\n",
        "\n",
        "    mock_document.assert_called_once()\n",
        "    # Check if add_heading and add_paragraph were called\n",
        "    mock_doc_instance.add_heading.assert_called_once()\n",
        "    assert mock_doc_instance.add_paragraph.call_count >= 1 # At least one paragraph for the data\n",
        "    mock_doc_instance.save.assert_called_once_with(docx_file)\n",
        "    assert result == docx_file\n",
        "\n",
        "@patch('__main__.save_html_as_image') # Mock the helper image saving function\n",
        "@patch('builtins.input', return_value='1') # Mock user input for single image choice\n",
        "def test_json_to_image_single(mock_input, mock_save_html_as_image, temp_dir):\n",
        "    json_file = create_dummy_json(os.path.join(temp_dir, \"test.json\"))\n",
        "    png_file = os.path.join(temp_dir, \"output.png\")\n",
        "\n",
        "    mock_save_html_as_image.return_value = png_file # Simulate success\n",
        "\n",
        "    result = json_to_image(json_file, png_file)\n",
        "\n",
        "    mock_save_html_as_image.assert_called_once_with(\n",
        "        json.dumps(json.load(open(json_file, encoding='utf-8')), indent=4, ensure_ascii=False),\n",
        "        png_file,\n",
        "        long_mode=True\n",
        "    )\n",
        "    assert result == png_file\n",
        "\n",
        "def test_json_to_txt(temp_dir):\n",
        "    json_file = create_dummy_json(os.path.join(temp_dir, \"test.json\"))\n",
        "    txt_file = os.path.join(temp_dir, \"output.txt\")\n",
        "\n",
        "    result = json_to_txt(json_file, txt_file)\n",
        "\n",
        "    assert os.path.exists(result)\n",
        "    with open(result, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "    # Check for pretty printed content\n",
        "    assert '\"name\": \"Alice\"' in content\n",
        "    assert '\"greeting\": \"नमस्ते\"' in content\n",
        "\n",
        "@patch('csv.DictWriter')\n",
        "def test_json_to_csv(mock_dict_writer, temp_dir):\n",
        "    json_file = create_dummy_json(os.path.join(temp_dir, \"test.json\"))\n",
        "    csv_file = os.path.join(temp_dir, \"output.csv\")\n",
        "\n",
        "    mock_writer_instance = MagicMock()\n",
        "    mock_dict_writer.return_value = mock_writer_instance\n",
        "\n",
        "    result = json_to_csv(json_file, csv_file)\n",
        "\n",
        "    mock_dict_writer.assert_called_once() # Should be called with fieldnames from the first object\n",
        "    mock_writer_instance.writeheader.assert_called_once()\n",
        "    assert mock_writer_instance.writerow.call_count == 2 # Called for each data object\n",
        "    assert result == csv_file\n",
        "\n",
        "@patch('openpyxl.Workbook')\n",
        "def test_json_to_xls(mock_workbook, temp_dir):\n",
        "    json_file = create_dummy_json(os.path.join(temp_dir, \"test.json\"))\n",
        "    xlsx_file = os.path.join(temp_dir, \"output.xlsx\")\n",
        "\n",
        "    mock_wb_instance = MagicMock()\n",
        "    mock_ws_instance = MagicMock()\n",
        "    mock_workbook.return_value = mock_wb_instance\n",
        "    mock_wb_instance.create_sheet.return_value = mock_ws_instance\n",
        "\n",
        "    result = json_to_xls(json_file, xlsx_file)\n",
        "\n",
        "    mock_workbook.assert_called_once()\n",
        "    mock_wb_instance.create_sheet.assert_called_once()\n",
        "    # Should append header and then each flattened row\n",
        "    assert mock_ws_instance.append.call_count == 3 # Header + 2 data rows\n",
        "    mock_wb_instance.save.assert_called_once_with(xlsx_file)\n",
        "    assert result == xlsx_file"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9CJH3er8eORM",
        "zj-3ywZ-eWsH",
        "_TdiKLeIenwu",
        "EdxE_LPGlZ9A",
        "_mLw5aAu5IHB",
        "RAk6v4g2exEN",
        "Cr-9rwy3sV3G",
        "rtwPABpdUbhM",
        "f14OZntcZJrz",
        "A4OP2ktc1g0t",
        "4a2ND6E0ZYK-",
        "lpgeFel2tW9d"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}